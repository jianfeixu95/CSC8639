{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 切换路径"
   ],
   "metadata": {
    "id": "9qBJGvf-wzj6"
   },
   "id": "9qBJGvf-wzj6"
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ryn9rGhjwEYH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1719930805988,
     "user_tz": -60,
     "elapsed": 1786,
     "user": {
      "displayName": "Jianfei Xu",
      "userId": "14747087902840301965"
     }
    },
    "outputId": "0708c97f-8188-45a5-fb3a-e82d64341c77"
   },
   "id": "Ryn9rGhjwEYH",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff1ad40d-bf35-444d-9867-5550f5577042",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff1ad40d-bf35-444d-9867-5550f5577042",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1719930805988,
     "user_tz": -60,
     "elapsed": 1,
     "user": {
      "displayName": "Jianfei Xu",
      "userId": "14747087902840301965"
     }
    },
    "outputId": "1eaa66b4-554b-481a-f6e9-9a8cc70af86e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The current working directory has been changed to: /content/drive/MyDrive/CSC8639/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "project_directory = '/content/drive/MyDrive/CSC8639/'\n",
    "\n",
    "if current_directory == project_directory:\n",
    "    print(\"The current working directory is already the specified directory, no change needed.\")\n",
    "else:\n",
    "    os.chdir(project_directory)\n",
    "    print(f\"The current working directory has been changed to: {project_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c053a87-826c-4043-8b5d-f3e41914335b",
   "metadata": {
    "id": "6c053a87-826c-4043-8b5d-f3e41914335b"
   },
   "source": [
    "# 1.数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180922e4-b6d9-43cb-9e15-8868382acb8a",
   "metadata": {
    "id": "180922e4-b6d9-43cb-9e15-8868382acb8a"
   },
   "source": [
    "## dcm文件转为jpg文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8396fe-2206-40f2-b7d4-357f6329c498",
   "metadata": {
    "id": "0a8396fe-2206-40f2-b7d4-357f6329c498",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1719830938489,
     "user_tz": -60,
     "elapsed": 526,
     "user": {
      "displayName": "Jianfei Xu",
      "userId": "14747087902840301965"
     }
    },
    "outputId": "f56d089a-162f-47c2-90e0-9560d540aa02"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydicom'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-9202de2478c4>\u001B[0m in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcv2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mpydicom\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpydicom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpixel_data_handlers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutil\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mapply_modality_lut\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mapply_voi_lut\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pydicom'",
      "",
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n"
     ],
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     }
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_modality_lut, apply_voi_lut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a10bbd26967e8",
   "metadata": {
    "id": "6f9a10bbd26967e8"
   },
   "outputs": [],
   "source": [
    "def resize_and_save(load_path, save_path):  # load_path=/path/to/load/*.dicom, save_path=/path/to/save/*.jpg\n",
    "    ds = pydicom.dcmread(load_path, force=True)\n",
    "    img = ds.pixel_array\n",
    "    img = apply_modality_lut(img, ds)  # rescaleSlope & intercept\n",
    "    img = apply_voi_lut(img, ds)  # windowing\n",
    "    if hasattr(ds, \"PhotometricInterpretation\"):\n",
    "        if ds.PhotometricInterpretation.lower().strip() == \"monochrome1\":\n",
    "            img = img.max() - img  # invert\n",
    "\n",
    "    h, w = img.shape\n",
    "    ratio = 512 / min(h, w)\n",
    "    target_size = (int(w * ratio), int(h * ratio))\n",
    "    img = cv2.resize(img, target_size, cv2.INTER_LANCZOS4)\n",
    "\n",
    "    # normalize\n",
    "    img = (img - img.min()) / (img.max() - img.min()) * np.iinfo(np.uint8).max\n",
    "    img = img.astype(np.uint8)\n",
    "    cv2.imwrite(save_path, img)\n",
    "\n",
    "\n",
    "def process_images_in_directory(load_path, save_path):\n",
    "    # Ensure the save_path directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # List all files in the load_path directory\n",
    "    files = os.listdir(load_path)\n",
    "\n",
    "    # Iterate over each file\n",
    "    for file in files:\n",
    "        if file.endswith('.dcm'):\n",
    "            # Construct full file paths\n",
    "            filename = os.path.basename(file)[:-4] + '.jpg'\n",
    "            input_file = os.path.join(load_path, file)\n",
    "            output_file = os.path.join(save_path, filename)\n",
    "\n",
    "            # Call resize_and_save function\n",
    "            resize_and_save(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed526e-8200-4ec4-9758-06358ccec7cc",
   "metadata": {
    "id": "eeed526e-8200-4ec4-9758-06358ccec7cc"
   },
   "outputs": [],
   "source": [
    "load_path = 'datasets/rsna/stage_2_train_images_mini'\n",
    "save_path = 'datasets/rsna/stage_2_train_images_jpg_mini'\n",
    "\n",
    "process_images_in_directory(load_path, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093debd-60a3-4397-993c-d00bb1859819",
   "metadata": {
    "id": "f093debd-60a3-4397-993c-d00bb1859819"
   },
   "outputs": [],
   "source": [
    "load_path = 'datasets/rsna/stage_2_test_images_mini'\n",
    "save_path = 'datasets/rsna/stage_2_test_images_jpg_mini'\n",
    "\n",
    "process_images_in_directory(load_path, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8701ebb-1197-4d33-9ca8-b74968e059c1",
   "metadata": {
    "id": "d8701ebb-1197-4d33-9ca8-b74968e059c1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e018131c-77cf-4139-b040-5dcd66f3c71c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "id": "e018131c-77cf-4139-b040-5dcd66f3c71c"
   },
   "source": [
    "## 划分pneumonia数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9074525-2a2d-48e9-89a6-ea23ac6f2e8c",
   "metadata": {
    "id": "b9074525-2a2d-48e9-89a6-ea23ac6f2e8c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import argparse\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "from gloria.constants import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def preprocess_pneumonia_data(test_fac=0.15):\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(PNEUMONIA_ORIGINAL_TRAIN_CSV)\n",
    "    except:\n",
    "        raise Exception(\n",
    "            \"Please make sure the the RSNA Pneumonia dataset is \\\n",
    "            stored at {PNEUMONIA_DATA_DIR}\"\n",
    "        )\n",
    "\n",
    "    # create bounding boxes\n",
    "    def create_bbox(row):\n",
    "        if row[\"Target\"] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            x1 = row[\"x\"]\n",
    "            y1 = row[\"y\"]\n",
    "            x2 = x1 + row[\"width\"]\n",
    "            y2 = y1 + row[\"height\"]\n",
    "            return [x1, y1, x2, y2]\n",
    "\n",
    "    df[\"bbox\"] = df.apply(lambda x: create_bbox(x), axis=1)\n",
    "\n",
    "    # aggregate multiple boxes\n",
    "    df = df[[\"patientId\", \"bbox\"]]\n",
    "    df = df.groupby(\"patientId\").agg(list)\n",
    "    df = df.reset_index()\n",
    "    df[\"bbox\"] = df[\"bbox\"].apply(lambda x: None if x == [0] else x)\n",
    "\n",
    "    # create labels\n",
    "    df[\"Target\"] = df[\"bbox\"].apply(lambda x: 0 if x == None else 1)\n",
    "\n",
    "    # no encoded pixels mean healthy\n",
    "    df[\"Path\"] = df[\"patientId\"].apply(lambda x: PNEUMONIA_IMG_DIR / (x + \".dcm\"))\n",
    "\n",
    "    # split data\n",
    "    train_df, test_val_df = train_test_split(df, test_size=test_fac * 2, random_state=0)\n",
    "    test_df, valid_df = train_test_split(test_val_df, test_size=0.5, random_state=0)\n",
    "\n",
    "    print(f\"Number of train samples: {len(train_df)}\")\n",
    "    print(train_df[\"Target\"].value_counts())\n",
    "    print(f\"Number of valid samples: {len(valid_df)}\")\n",
    "    print(valid_df[\"Target\"].value_counts())\n",
    "    print(f\"Number of test samples: {len(test_df)}\")\n",
    "    print(test_df[\"Target\"].value_counts())\n",
    "\n",
    "    train_df.to_csv(PNEUMONIA_TRAIN_CSV)\n",
    "    valid_df.to_csv(PNEUMONIA_VALID_CSV)\n",
    "    test_df.to_csv(PNEUMONIA_TEST_CSV)\n",
    "\n",
    "\n",
    "def preprocess_pneumothorax_data(test_fac=0.15):\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(PNEUMOTHORAX_ORIGINAL_TRAIN_CSV)\n",
    "    except:\n",
    "        raise Exception(\n",
    "            \"Please make sure the the SIIM Pneumothorax dataset is \\\n",
    "            stored at {PNEUMOTHORAX_DATA_DIR}\"\n",
    "        )\n",
    "\n",
    "    # get image paths\n",
    "    img_paths = {}\n",
    "    for subdir, dirs, files in tqdm.tqdm(os.walk(PNEUMOTHORAX_IMG_DIR)):\n",
    "        for f in files:\n",
    "            if \"dcm\" in f:\n",
    "                # remove dcm\n",
    "                file_id = f[:-4]\n",
    "                img_paths[file_id] = os.path.join(subdir, f)\n",
    "\n",
    "    # no encoded pixels mean healthy\n",
    "    df[\"Label\"] = df.apply(\n",
    "        lambda x: 0.0 if x[\" EncodedPixels\"] == \" -1\" else 1.0, axis=1\n",
    "    )\n",
    "    df[\"Path\"] = df[\"ImageId\"].apply(lambda x: img_paths[x])\n",
    "\n",
    "    # split data\n",
    "    train_df, test_val_df = train_test_split(df, test_size=test_fac * 2, random_state=0)\n",
    "    test_df, valid_df = train_test_split(test_val_df, test_size=0.5, random_state=0)\n",
    "\n",
    "    print(f\"Number of train samples: {len(train_df)}\")\n",
    "    print(train_df[\"Label\"].value_counts())\n",
    "    print(f\"Number of valid samples: {len(valid_df)}\")\n",
    "    print(valid_df[\"Label\"].value_counts())\n",
    "    print(f\"Number of test samples: {len(test_df)}\")\n",
    "    print(test_df[\"Label\"].value_counts())\n",
    "\n",
    "    train_df.to_csv(PNEUMOTHORAX_TRAIN_CSV)\n",
    "    valid_df.to_csv(PNEUMOTHORAX_VALID_CSV)\n",
    "    test_df.to_csv(PNEUMOTHORAX_TEST_CSV)\n",
    "\n",
    "\n",
    "def preprocess_chexpert_5x200_data():\n",
    "\n",
    "    df = pd.read_csv(CHEXPERT_ORIGINAL_TRAIN_CSV)\n",
    "    df = df.fillna(0)\n",
    "    df = df[df[\"Frontal/Lateral\"] == \"Frontal\"]\n",
    "\n",
    "    df_master = pd.read_csv(CHEXPERT_MASTER_CSV)\n",
    "    df_master = df_master[[\"Path\", \"Report Impression\"]]\n",
    "\n",
    "    task_dfs = []\n",
    "    for i, t in enumerate(CHEXPERT_COMPETITION_TASKS):\n",
    "        index = np.zeros(14)\n",
    "        index[i] = 1\n",
    "        df_task = df[\n",
    "            (df[\"Atelectasis\"] == index[0])\n",
    "            & (df[\"Cardiomegaly\"] == index[1])\n",
    "            & (df[\"Consolidation\"] == index[2])\n",
    "            & (df[\"Edema\"] == index[3])\n",
    "            & (df[\"Pleural Effusion\"] == index[4])\n",
    "            & (df[\"Enlarged Cardiomediastinum\"] == index[5])\n",
    "            & (df[\"Lung Lesion\"] == index[7])\n",
    "            & (df[\"Lung Opacity\"] == index[8])\n",
    "            & (df[\"Pneumonia\"] == index[9])\n",
    "            & (df[\"Pneumothorax\"] == index[10])\n",
    "            & (df[\"Pleural Other\"] == index[11])\n",
    "            & (df[\"Fracture\"] == index[12])\n",
    "            & (df[\"Support Devices\"] == index[13])\n",
    "        ]\n",
    "        df_task = df_task.sample(n=200)\n",
    "        task_dfs.append(df_task)\n",
    "    df_200 = pd.concat(task_dfs)\n",
    "\n",
    "    # get reports\n",
    "    df_200 = pd.merge(df_200, df_master, how=\"left\", left_on=\"Path\", right_on=\"Path\")\n",
    "\n",
    "    return df_200\n",
    "\n",
    "\n",
    "def preprocess_chexpert_data():\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(CHEXPERT_ORIGINAL_TRAIN_CSV)\n",
    "    except:\n",
    "        raise Exception(\n",
    "            \"Please make sure the the Pneunotrhoax dataset is \\\n",
    "            stored at {PNEUMOTHORAX_DATA_DIR}\"\n",
    "        )\n",
    "\n",
    "    df_200 = preprocess_chexpert_5x200_data()\n",
    "    df = df[~df[CHEXPERT_PATH_COL].isin(df_200[CHEXPERT_PATH_COL])]\n",
    "    valid_ids = np.random.choice(len(df), size=CHEXPERT_VALID_NUM, replace=False)\n",
    "    valid_df = df.iloc[valid_ids]\n",
    "    train_df = df.drop(valid_ids, errors=\"ignore\")\n",
    "\n",
    "    print(f\"Number of train samples: {len(train_df)}\")\n",
    "    print(f\"Number of valid samples: {len(valid_df)}\")\n",
    "    print(f\"Number of chexpert5x200 samples: {len(df_200)}\")\n",
    "\n",
    "    train_df.to_csv(CHEXPERT_TRAIN_CSV)\n",
    "    valid_df.to_csv(CHEXPERT_VALID_CSV)\n",
    "    df_200.to_csv(CHEXPERT_5x200)\n",
    "\n",
    "\n",
    "_DATASETS = {\n",
    "    \"chexpert\": preprocess_chexpert_data,\n",
    "    \"pneumonia\": preprocess_pneumonia_data,\n",
    "    \"pneumothorax\": preprocess_pneumothorax_data,\n",
    "}\n",
    "\n",
    "\n",
    "def available_datasets():\n",
    "    \"\"\"Returns the names of available datasets\"\"\"\n",
    "    return list(_DATASETS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ebe456-6677-4804-ae3e-e68a79338391",
   "metadata": {
    "id": "c9ebe456-6677-4804-ae3e-e68a79338391"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"-d\",\n",
    "    \"--dataset\",\n",
    "    type=str,\n",
    "    help=\"dataset type, one of [chexpert, pneumonia, pneumothorax]\",\n",
    "    required=True,\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.dataset.lower() in _DATASETS.keys():\n",
    "    _DATASETS[args.dataset.lower()]()\n",
    "else:\n",
    "    RuntimeError(\n",
    "        f\"Model {args.dataset} not found; available datasets = {available_datasets()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c70f09-9141-4cb7-9d00-f3aa5d737a4b",
   "metadata": {
    "id": "91c70f09-9141-4cb7-9d00-f3aa5d737a4b"
   },
   "source": [
    "## 划分mimic-cxr-jpg数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a40ed-323b-4db4-858f-912db2a9a179",
   "metadata": {
    "id": "d02a40ed-323b-4db4-858f-912db2a9a179"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "# 定义函数生成图像路径\n",
    "def generate_image_path(row):\n",
    "    subject_id = row['subject_id']\n",
    "    study_id = row['study_id']\n",
    "    dicom_id = row['dicom_id']\n",
    "\n",
    "    subject_folder = \"p\" + str(subject_id).zfill(8)[:2]\n",
    "    subject_subfolder = \"p\" + str(subject_id).zfill(8)\n",
    "    study_folder = \"s\" + str(study_id)\n",
    "\n",
    "    image_path = os.path.join(jpg_root_dir, subject_folder, subject_subfolder, study_folder, dicom_id + \".jpg\")\n",
    "    return image_path\n",
    "\n",
    "\n",
    "def extract_findings_and_impression(row):\n",
    "    findings = []\n",
    "    impression = []\n",
    "\n",
    "    subject_id = row['subject_id']\n",
    "    study_id = row['study_id']\n",
    "    subject_folder = \"p\" + str(subject_id).zfill(8)[:2]\n",
    "    subject_subfolder = \"p\" + str(subject_id).zfill(8)\n",
    "    study_folder = \"s\" + str(study_id)\n",
    "    text_path = os.path.join(text_root_dir, subject_folder, subject_subfolder, study_folder + \".txt\")\n",
    "\n",
    "    with open(text_path, 'r') as file:\n",
    "        lines = file.read()\n",
    "        find_findings = False\n",
    "        find_impression = False\n",
    "\n",
    "        findings_pattern = re.compile(r'FINDINGS:\\s*(.*?)\\s*IMPRESSION:', re.DOTALL)\n",
    "        impression_pattern = re.compile(r'IMPRESSION:\\s*(.*)', re.DOTALL)\n",
    "\n",
    "        findings_match = findings_pattern.search(lines)\n",
    "        impression_match = impression_pattern.search(lines)\n",
    "\n",
    "    findings_text = findings_match.group(1).replace('\\n', ' ').strip() if findings_match else None\n",
    "    impression_text = impression_match.group(1).replace('\\n', ' ').strip() if impression_match else None\n",
    "\n",
    "    if findings_text is None or impression_text is None:\n",
    "        return None\n",
    "\n",
    "    return [findings_text, impression_text]\n",
    "\n",
    "# 定义函数：合并并去除重复值和空值\n",
    "def merge_unique(values):\n",
    "    merged_list = []\n",
    "    for value in values:\n",
    "        if pd.notna(value):\n",
    "            merged_list.append(value)\n",
    "\n",
    "    return list(set(merged_list)) if merged_list else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da108c7-52aa-499d-86c2-988729a8d7c9",
   "metadata": {
    "id": "9da108c7-52aa-499d-86c2-988729a8d7c9"
   },
   "outputs": [],
   "source": [
    "# 定义根目录\n",
    "jpg_root_dir = \"/home/jovyan/work/CSC8639/mimic-cxr-jpg-v2.1.0/Data/files\"\n",
    "text_root_dir = \"/home/jovyan/work/CSC8639/mimic-cxr/2.0.0/files\"\n",
    "\n",
    "# 读取CSV文件\n",
    "metadata_path = \"datasets/mimic-cxr-jpg/mimic-cxr-2.0.0-metadata.csv\"  # 替换为你的文件路径\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# 生成新的image列\n",
    "metadata['image'] = metadata.apply(generate_image_path, axis=1)\n",
    "metadata['AP'] = metadata['image'].where(metadata['ViewPosition'] == 'AP', None)\n",
    "metadata['PA'] = metadata['image'].where(metadata['ViewPosition'] == 'PA', None)\n",
    "metadata['Lateral'] = metadata['image'].where(metadata['ViewPosition'] == 'LATERAL', None)\n",
    "metadata['view'] = metadata.apply(lambda metadata: metadata['ViewPosition'] if metadata['ViewPosition'] in [\"AP\",\"PA\",\"LATERAL\"] else None, axis=1)\n",
    "metadata['text'] = metadata.apply(extract_findings_and_impression, axis=1)\n",
    "\n",
    "# 保存到新的CSV文件\n",
    "new_metadata_path = \"datasets/mimic-cxr-jpg/mimic-cxr-2.0.0-metadata-with-images.csv\"  # 替换为你希望保存的路径\n",
    "metadata.to_csv(new_metadata_path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fdf5d-afb0-4a6d-b393-ade6297786ef",
   "metadata": {
    "id": "726fdf5d-afb0-4a6d-b393-ade6297786ef"
   },
   "outputs": [],
   "source": [
    "# 读取数据文件\n",
    "file_path = \"datasets/mimic-cxr-jpg/mimic-cxr-2.0.0-metadata-with-images.csv\"\n",
    "metadata = pd.read_csv(file_path)\n",
    "\n",
    "# 按照 study_id 列进行分组，并合并其他列数据为列表\n",
    "grouped = metadata.groupby('study_id').agg({\n",
    "    'image': lambda x: merge_unique(x),\n",
    "    'view': lambda x: merge_unique(x),\n",
    "    'AP': lambda x: merge_unique(x),\n",
    "    'PA': lambda x: merge_unique(x),\n",
    "    'Lateral': lambda x: merge_unique(x),\n",
    "    'text': 'first'\n",
    "}).reset_index()\n",
    "grouped = grouped[grouped['text'].notna()]\n",
    "\n",
    "# 将处理后的数据保存到新的CSV文件中\n",
    "output_file = 'datasets/mimic-cxr-jpg/mimic-cxr-grouped-metadata.csv'\n",
    "grouped.to_csv(output_file, index=True)\n",
    "print(f\"Saved grouped metadata to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55d4f8-74b6-4227-b7bb-ac99a4a54145",
   "metadata": {
    "id": "2e55d4f8-74b6-4227-b7bb-ac99a4a54145"
   },
   "outputs": [],
   "source": [
    "# 读取 metadata 和 split 数据\n",
    "metadata = pd.read_csv('datasets/mimic-cxr-jpg/mimic-cxr-grouped-metadata.csv')\n",
    "split_data = pd.read_csv('datasets/mimic-cxr-jpg/mimic-cxr-2.0.0-split.csv')\n",
    "\n",
    "# 将 split 数据和 metadata 数据合并，根据 study_id 进行匹配\n",
    "merged_data = pd.merge(metadata, split_data, on = 'study_id')\n",
    "\n",
    "# 拆分成 train, validate, test 数据\n",
    "train_data = merged_data[merged_data['split'] == 'train']\n",
    "valid_data = merged_data[merged_data['split'] == 'validate']\n",
    "test_data = merged_data[merged_data['split'] == 'test']\n",
    "\n",
    "cols_to_save = ['image', 'view', 'AP', 'PA', 'Lateral', 'text']\n",
    "\n",
    "# 保存为新文件\n",
    "train_data[cols_to_save].to_csv('datasets/mimic-cxr-jpg/mimic_train.csv', index=True)\n",
    "valid_data[cols_to_save].to_csv('datasets/mimic-cxr-jpg/mimic_valid.csv', index=True)\n",
    "test_data[cols_to_save].to_csv('datasets/mimic-cxr-jpg/mimic_test.csv', index=True)\n",
    "\n",
    "if os.path.exists(new_metadata_path):\n",
    "    os.remove(new_metadata_path)\n",
    "    print(f\"{new_metadata_path} has been deleted.\")\n",
    "else:\n",
    "    print(f\"{new_metadata_path} does not exist.\")\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "    print(f\"{output_file} has been deleted.\")\n",
    "else:\n",
    "    print(f\"{output_file} does not exist.\")\n",
    "\n",
    "print(\"Files saved successfully: mimic_train.csv, mimic_valid.csv, mimic_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146cbd58-67f2-4fc6-8560-877e4a39f86d",
   "metadata": {
    "id": "146cbd58-67f2-4fc6-8560-877e4a39f86d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc6517ea-df4d-4dfb-8ad7-9f2fc99f0aff",
   "metadata": {
    "id": "dc6517ea-df4d-4dfb-8ad7-9f2fc99f0aff"
   },
   "source": [
    "## mimic-cxr-jpg backtranslation增强文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d06a875-b841-4551-9d6e-ae23fd23ef62",
   "metadata": {
    "id": "6d06a875-b841-4551-9d6e-ae23fd23ef62"
   },
   "outputs": [],
   "source": [
    "!python \"text_augmentation/back_translation.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb384f4-5798-4ca7-8f43-f05f737865c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "id": "ceb384f4-5798-4ca7-8f43-f05f737865c6"
   },
   "source": [
    "# 2.安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b9ef06-5f41-4e75-80ff-865048aa5346",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "98b9ef06-5f41-4e75-80ff-865048aa5346",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1719923098639,
     "user_tz": -60,
     "elapsed": 282452,
     "user": {
      "displayName": "Jianfei Xu",
      "userId": "14747087902840301965"
     }
    },
    "outputId": "01f35cc0-6bbc-419e-989a-22af1f3be7ba"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: albumentations==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 2)) (1.3.1)\n",
      "Collecting antlr4-python3-runtime==4.9.3 (from -r Model/image-classification-model/requirements.txt (line 3))\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m117.0/117.0 kB\u001B[0m \u001B[31m3.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: anyio==3.7.1 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 4)) (3.7.1)\n",
      "Collecting argon2-cffi==21.3.0 (from -r Model/image-classification-model/requirements.txt (line 5))\n",
      "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 6)) (21.2.0)\n",
      "Collecting arrow==1.2.3 (from -r Model/image-classification-model/requirements.txt (line 7))\n",
      "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m66.4/66.4 kB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting asttokens==2.0.5 (from -r Model/image-classification-model/requirements.txt (line 8))\n",
      "  Downloading asttokens-2.0.5-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 9)) (1.6.3)\n",
      "Collecting async-lru==2.0.3 (from -r Model/image-classification-model/requirements.txt (line 10))\n",
      "  Downloading async_lru-2.0.3-py3-none-any.whl (6.0 kB)\n",
      "Collecting attrs==23.1.0 (from -r Model/image-classification-model/requirements.txt (line 11))\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.2/61.2 kB\u001B[0m \u001B[31m6.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting Babel==2.12.1 (from -r Model/image-classification-model/requirements.txt (line 12))\n",
      "  Downloading Babel-2.12.1-py3-none-any.whl (10.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.1/10.1 MB\u001B[0m \u001B[31m23.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 13)) (0.2.0)\n",
      "Collecting beautifulsoup4==4.12.2 (from -r Model/image-classification-model/requirements.txt (line 14))\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m143.0/143.0 kB\u001B[0m \u001B[31m13.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting bleach==6.0.0 (from -r Model/image-classification-model/requirements.txt (line 15))\n",
      "  Downloading bleach-6.0.0-py3-none-any.whl (162 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m162.5/162.5 kB\u001B[0m \u001B[31m15.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting boltons==23.0.0 (from -r Model/image-classification-model/requirements.txt (line 16))\n",
      "  Downloading boltons-23.0.0-py2.py3-none-any.whl (194 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.8/194.8 kB\u001B[0m \u001B[31m17.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting brotlipy==0.7.0 (from -r Model/image-classification-model/requirements.txt (line 17))\n",
      "  Downloading brotlipy-0.7.0-cp35-abi3-manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m42.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting cachetools==5.3.1 (from -r Model/image-classification-model/requirements.txt (line 18))\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting certifi==2023.5.7 (from -r Model/image-classification-model/requirements.txt (line 19))\n",
      "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m157.0/157.0 kB\u001B[0m \u001B[31m17.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting cffi==1.15.1 (from -r Model/image-classification-model/requirements.txt (line 20))\n",
      "  Downloading cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m441.8/441.8 kB\u001B[0m \u001B[31m37.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting cfgv==3.3.1 (from -r Model/image-classification-model/requirements.txt (line 21))\n",
      "  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
      "Collecting chardet==3.0.4 (from -r Model/image-classification-model/requirements.txt (line 22))\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m133.4/133.4 kB\u001B[0m \u001B[31m14.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting charset-normalizer==2.0.4 (from -r Model/image-classification-model/requirements.txt (line 23))\n",
      "  Downloading charset_normalizer-2.0.4-py3-none-any.whl (36 kB)\n",
      "Collecting click==8.1.6 (from -r Model/image-classification-model/requirements.txt (line 24))\n",
      "  Downloading click-8.1.6-py3-none-any.whl (97 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m97.9/97.9 kB\u001B[0m \u001B[31m10.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting comm==0.1.3 (from -r Model/image-classification-model/requirements.txt (line 25))\n",
      "  Downloading comm-0.1.3-py3-none-any.whl (6.6 kB)\n",
      "Collecting contourpy==1.1.0 (from -r Model/image-classification-model/requirements.txt (line 26))\n",
      "  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m300.7/300.7 kB\u001B[0m \u001B[31m20.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting cryptography==39.0.1 (from -r Model/image-classification-model/requirements.txt (line 27))\n",
      "  Downloading cryptography-39.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.2/4.2 MB\u001B[0m \u001B[31m51.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting cycler==0.11.0 (from -r Model/image-classification-model/requirements.txt (line 28))\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting debugpy==1.6.7 (from -r Model/image-classification-model/requirements.txt (line 29))\n",
      "  Downloading debugpy-1.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m46.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting decorator==5.1.1 (from -r Model/image-classification-model/requirements.txt (line 30))\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 31)) (0.7.1)\n",
      "Collecting distlib==0.3.7 (from -r Model/image-classification-model/requirements.txt (line 32))\n",
      "  Downloading distlib-0.3.7-py2.py3-none-any.whl (468 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m468.9/468.9 kB\u001B[0m \u001B[31m43.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting dnspython==2.3.0 (from -r Model/image-classification-model/requirements.txt (line 33))\n",
      "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m283.7/283.7 kB\u001B[0m \u001B[31m23.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting einops==0.6.1 (from -r Model/image-classification-model/requirements.txt (line 34))\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.2/42.2 kB\u001B[0m \u001B[31m5.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting exceptiongroup==1.1.1 (from -r Model/image-classification-model/requirements.txt (line 35))\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Collecting executing==0.8.3 (from -r Model/image-classification-model/requirements.txt (line 36))\n",
      "  Downloading executing-0.8.3-py2.py3-none-any.whl (16 kB)\n",
      "Collecting expecttest==0.1.4 (from -r Model/image-classification-model/requirements.txt (line 37))\n",
      "  Downloading expecttest-0.1.4-py3-none-any.whl (6.5 kB)\n",
      "Collecting fastjsonschema==2.17.1 (from -r Model/image-classification-model/requirements.txt (line 38))\n",
      "  Downloading fastjsonschema-2.17.1-py3-none-any.whl (23 kB)\n",
      "Collecting filelock==3.12.2 (from -r Model/image-classification-model/requirements.txt (line 39))\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting fonttools==4.41.1 (from -r Model/image-classification-model/requirements.txt (line 40))\n",
      "  Downloading fonttools-4.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.5/4.5 MB\u001B[0m \u001B[31m22.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting fqdn==1.5.1 (from -r Model/image-classification-model/requirements.txt (line 41))\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: fsspec==2023.6.0 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 42)) (2023.6.0)\n",
      "Collecting gensim==4.3.1 (from -r Model/image-classification-model/requirements.txt (line 43))\n",
      "  Downloading gensim-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.4/26.4 MB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: glob2==0.7 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 44)) (0.7)\n",
      "Collecting gmpy2==2.1.2 (from -r Model/image-classification-model/requirements.txt (line 45))\n",
      "  Downloading gmpy2-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m52.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting google-auth==2.22.0 (from -r Model/image-classification-model/requirements.txt (line 46))\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m181.8/181.8 kB\u001B[0m \u001B[31m19.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting google-auth-oauthlib==1.0.0 (from -r Model/image-classification-model/requirements.txt (line 47))\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting googletrans==3.0.0 (from -r Model/image-classification-model/requirements.txt (line 48))\n",
      "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting grpcio==1.56.2 (from -r Model/image-classification-model/requirements.txt (line 49))\n",
      "  Downloading grpcio-1.56.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.2/5.2 MB\u001B[0m \u001B[31m9.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting h11==0.9.0 (from -r Model/image-classification-model/requirements.txt (line 50))\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m53.6/53.6 kB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting h2==3.2.0 (from -r Model/image-classification-model/requirements.txt (line 51))\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m65.0/65.0 kB\u001B[0m \u001B[31m563.8 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting hpack==3.0.0 (from -r Model/image-classification-model/requirements.txt (line 52))\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting hstspreload==2023.1.1 (from -r Model/image-classification-model/requirements.txt (line 53))\n",
      "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m72.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting httpcore==0.9.1 (from -r Model/image-classification-model/requirements.txt (line 54))\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.6/42.6 kB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting httpx==0.13.3 (from -r Model/image-classification-model/requirements.txt (line 55))\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.1/55.1 kB\u001B[0m \u001B[31m7.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting huggingface-hub==0.16.4 (from -r Model/image-classification-model/requirements.txt (line 56))\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m268.8/268.8 kB\u001B[0m \u001B[31m28.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting hydra-core==1.3.2 (from -r Model/image-classification-model/requirements.txt (line 57))\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m154.5/154.5 kB\u001B[0m \u001B[31m17.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting hyperframe==5.2.0 (from -r Model/image-classification-model/requirements.txt (line 58))\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting hypothesis==6.75.2 (from -r Model/image-classification-model/requirements.txt (line 59))\n",
      "  Downloading hypothesis-6.75.2-py3-none-any.whl (413 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m413.5/413.5 kB\u001B[0m \u001B[31m28.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting identify==2.5.26 (from -r Model/image-classification-model/requirements.txt (line 60))\n",
      "  Downloading identify-2.5.26-py2.py3-none-any.whl (98 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m98.8/98.8 kB\u001B[0m \u001B[31m13.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting idna==2.10 (from -r Model/image-classification-model/requirements.txt (line 61))\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.8/58.8 kB\u001B[0m \u001B[31m7.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting imageio==2.31.1 (from -r Model/image-classification-model/requirements.txt (line 62))\n",
      "  Downloading imageio-2.31.1-py3-none-any.whl (313 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m313.2/313.2 kB\u001B[0m \u001B[31m31.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting ipykernel==6.24.0 (from -r Model/image-classification-model/requirements.txt (line 63))\n",
      "  Downloading ipykernel-6.24.0-py3-none-any.whl (152 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m152.8/152.8 kB\u001B[0m \u001B[31m18.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting ipython==8.12.0 (from -r Model/image-classification-model/requirements.txt (line 64))\n",
      "  Downloading ipython-8.12.0-py3-none-any.whl (796 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m796.4/796.4 kB\u001B[0m \u001B[31m46.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting isoduration==20.11.0 (from -r Model/image-classification-model/requirements.txt (line 65))\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting jedi==0.18.1 (from -r Model/image-classification-model/requirements.txt (line 66))\n",
      "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m63.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting Jinja2==3.1.2 (from -r Model/image-classification-model/requirements.txt (line 67))\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m133.1/133.1 kB\u001B[0m \u001B[31m17.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting joblib==1.3.1 (from -r Model/image-classification-model/requirements.txt (line 68))\n",
      "  Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m302.0/302.0 kB\u001B[0m \u001B[31m31.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting json5==0.9.14 (from -r Model/image-classification-model/requirements.txt (line 69))\n",
      "  Downloading json5-0.9.14-py2.py3-none-any.whl (19 kB)\n",
      "Collecting jsonpatch==1.32 (from -r Model/image-classification-model/requirements.txt (line 70))\n",
      "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
      "Collecting jsonpointer==2.1 (from -r Model/image-classification-model/requirements.txt (line 71))\n",
      "  Downloading jsonpointer-2.1-py2.py3-none-any.whl (7.4 kB)\n",
      "Collecting jsonschema==4.18.4 (from -r Model/image-classification-model/requirements.txt (line 72))\n",
      "  Downloading jsonschema-4.18.4-py3-none-any.whl (80 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m81.0/81.0 kB\u001B[0m \u001B[31m10.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting jsonschema-specifications==2023.7.1 (from -r Model/image-classification-model/requirements.txt (line 73))\n",
      "  Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
      "Collecting jupyter_client==8.3.0 (from -r Model/image-classification-model/requirements.txt (line 74))\n",
      "  Downloading jupyter_client-8.3.0-py3-none-any.whl (103 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m103.2/103.2 kB\u001B[0m \u001B[31m10.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting jupyter_core==5.3.1 (from -r Model/image-classification-model/requirements.txt (line 75))\n",
      "  Downloading jupyter_core-5.3.1-py3-none-any.whl (93 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m93.7/93.7 kB\u001B[0m \u001B[31m13.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting jupyter-events==0.6.3 (from -r Model/image-classification-model/requirements.txt (line 76))\n",
      "  Downloading jupyter_events-0.6.3-py3-none-any.whl (18 kB)\n",
      "Collecting jupyter-lsp==2.2.0 (from -r Model/image-classification-model/requirements.txt (line 77))\n",
      "  Downloading jupyter_lsp-2.2.0-py3-none-any.whl (65 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m66.0/66.0 kB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting jupyter_server==2.7.0 (from -r Model/image-classification-model/requirements.txt (line 78))\n",
      "  Downloading jupyter_server-2.7.0-py3-none-any.whl (375 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m375.1/375.1 kB\u001B[0m \u001B[31m33.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting jupyter_server_terminals==0.4.4 (from -r Model/image-classification-model/requirements.txt (line 79))\n",
      "  Downloading jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
      "Collecting jupyterlab==4.0.3 (from -r Model/image-classification-model/requirements.txt (line 80))\n",
      "  Downloading jupyterlab-4.0.3-py3-none-any.whl (9.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.2/9.2 MB\u001B[0m \u001B[31m108.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting jupyterlab-pygments==0.2.2 (from -r Model/image-classification-model/requirements.txt (line 81))\n",
      "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting jupyterlab_server==2.23.0 (from -r Model/image-classification-model/requirements.txt (line 82))\n",
      "  Downloading jupyterlab_server-2.23.0-py3-none-any.whl (57 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.3/57.3 kB\u001B[0m \u001B[31m7.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting kiwisolver==1.4.4 (from -r Model/image-classification-model/requirements.txt (line 83))\n",
      "  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m58.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting lazy_loader==0.3 (from -r Model/image-classification-model/requirements.txt (line 84))\n",
      "  Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Collecting libarchive-c==2.9 (from -r Model/image-classification-model/requirements.txt (line 85))\n",
      "  Downloading libarchive_c-2.9-py2.py3-none-any.whl (11 kB)\n",
      "Collecting Markdown==3.4.3 (from -r Model/image-classification-model/requirements.txt (line 86))\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m93.9/93.9 kB\u001B[0m \u001B[31m13.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting MarkupSafe==2.1.1 (from -r Model/image-classification-model/requirements.txt (line 87))\n",
      "  Downloading MarkupSafe-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting matplotlib==3.7.2 (from -r Model/image-classification-model/requirements.txt (line 88))\n",
      "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.6/11.6 MB\u001B[0m \u001B[31m62.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting matplotlib-inline==0.1.6 (from -r Model/image-classification-model/requirements.txt (line 89))\n",
      "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "Collecting mistune==3.0.1 (from -r Model/image-classification-model/requirements.txt (line 90))\n",
      "  Downloading mistune-3.0.1-py3-none-any.whl (47 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m48.0/48.0 kB\u001B[0m \u001B[31m6.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 91)) (1.3.0)\n",
      "Collecting nbclient==0.8.0 (from -r Model/image-classification-model/requirements.txt (line 92))\n",
      "  Downloading nbclient-0.8.0-py3-none-any.whl (73 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m73.1/73.1 kB\u001B[0m \u001B[31m9.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nbconvert==7.7.2 (from -r Model/image-classification-model/requirements.txt (line 93))\n",
      "  Downloading nbconvert-7.7.2-py3-none-any.whl (254 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m254.6/254.6 kB\u001B[0m \u001B[31m25.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nbformat==5.9.1 (from -r Model/image-classification-model/requirements.txt (line 94))\n",
      "  Downloading nbformat-5.9.1-py3-none-any.whl (77 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m77.6/77.6 kB\u001B[0m \u001B[31m9.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nest-asyncio==1.5.6 (from -r Model/image-classification-model/requirements.txt (line 95))\n",
      "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Collecting networkx==3.1 (from -r Model/image-classification-model/requirements.txt (line 96))\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m73.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 97)) (3.8.1)\n",
      "Collecting nodeenv==1.8.0 (from -r Model/image-classification-model/requirements.txt (line 98))\n",
      "  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n",
      "Collecting notebook_shim==0.2.3 (from -r Model/image-classification-model/requirements.txt (line 99))\n",
      "  Downloading notebook_shim-0.2.3-py3-none-any.whl (13 kB)\n",
      "Collecting numpy==1.24.3 (from -r Model/image-classification-model/requirements.txt (line 100))\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m17.3/17.3 MB\u001B[0m \u001B[31m53.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: oauthlib==3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 101)) (3.2.2)\n",
      "Collecting omegaconf==2.3.0 (from -r Model/image-classification-model/requirements.txt (line 102))\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m79.5/79.5 kB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting opencv-python-headless==4.8.0.74 (from -r Model/image-classification-model/requirements.txt (line 103))\n",
      "  Downloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.1/49.1 MB\u001B[0m \u001B[31m11.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting overrides==7.3.1 (from -r Model/image-classification-model/requirements.txt (line 104))\n",
      "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
      "Collecting packaging==23.0 (from -r Model/image-classification-model/requirements.txt (line 105))\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.7/42.7 kB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 106)) (2.0.3)\n",
      "Collecting pandocfilters==1.5.0 (from -r Model/image-classification-model/requirements.txt (line 107))\n",
      "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting parso==0.8.3 (from -r Model/image-classification-model/requirements.txt (line 108))\n",
      "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m100.8/100.8 kB\u001B[0m \u001B[31m15.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pexpect==4.8.0 (from -r Model/image-classification-model/requirements.txt (line 109))\n",
      "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m59.0/59.0 kB\u001B[0m \u001B[31m9.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 110)) (0.7.5)\n",
      "Requirement already satisfied: Pillow==9.4.0 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 111)) (9.4.0)\n",
      "Collecting pip==23.0.1 (from -r Model/image-classification-model/requirements.txt (line 112))\n",
      "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m93.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pkginfo==1.9.6 (from -r Model/image-classification-model/requirements.txt (line 113))\n",
      "  Downloading pkginfo-1.9.6-py3-none-any.whl (30 kB)\n",
      "Collecting platformdirs==3.9.1 (from -r Model/image-classification-model/requirements.txt (line 114))\n",
      "  Downloading platformdirs-3.9.1-py3-none-any.whl (16 kB)\n",
      "Collecting pluggy==1.0.0 (from -r Model/image-classification-model/requirements.txt (line 115))\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting pre-commit==3.3.3 (from -r Model/image-classification-model/requirements.txt (line 116))\n",
      "  Downloading pre_commit-3.3.3-py2.py3-none-any.whl (202 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m202.8/202.8 kB\u001B[0m \u001B[31m30.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting prometheus-client==0.17.1 (from -r Model/image-classification-model/requirements.txt (line 117))\n",
      "  Downloading prometheus_client-0.17.1-py3-none-any.whl (60 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m60.6/60.6 kB\u001B[0m \u001B[31m7.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting prompt-toolkit==3.0.36 (from -r Model/image-classification-model/requirements.txt (line 118))\n",
      "  Downloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m386.4/386.4 kB\u001B[0m \u001B[31m52.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting protobuf==4.23.4 (from -r Model/image-classification-model/requirements.txt (line 119))\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m304.5/304.5 kB\u001B[0m \u001B[31m40.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting psutil==5.9.0 (from -r Model/image-classification-model/requirements.txt (line 120))\n",
      "  Downloading psutil-5.9.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m281.4/281.4 kB\u001B[0m \u001B[31m37.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 121)) (0.7.0)\n",
      "Collecting pure-eval==0.2.2 (from -r Model/image-classification-model/requirements.txt (line 122))\n",
      "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1==0.5.0 (from -r Model/image-classification-model/requirements.txt (line 123))\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m83.9/83.9 kB\u001B[0m \u001B[31m10.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pyasn1-modules==0.3.0 (from -r Model/image-classification-model/requirements.txt (line 124))\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m181.3/181.3 kB\u001B[0m \u001B[31m23.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pycosat==0.6.6 (from -r Model/image-classification-model/requirements.txt (line 125))\n",
      "  Downloading pycosat-0.6.6.tar.gz (71 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m71.6/71.6 kB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting pycparser==2.21 (from -r Model/image-classification-model/requirements.txt (line 126))\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m118.7/118.7 kB\u001B[0m \u001B[31m21.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting Pygments==2.15.1 (from -r Model/image-classification-model/requirements.txt (line 127))\n",
      "  Downloading Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m75.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pynvml==11.5.0 (from -r Model/image-classification-model/requirements.txt (line 128))\n",
      "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m53.1/53.1 kB\u001B[0m \u001B[31m8.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pyOpenSSL==23.0.0 (from -r Model/image-classification-model/requirements.txt (line 129))\n",
      "  Downloading pyOpenSSL-23.0.0-py3-none-any.whl (57 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.3/57.3 kB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pyparsing==3.0.9 (from -r Model/image-classification-model/requirements.txt (line 130))\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m98.3/98.3 kB\u001B[0m \u001B[31m12.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: PySocks==1.7.1 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 131)) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 132)) (2.8.2)\n",
      "Collecting python-etcd==0.4.5 (from -r Model/image-classification-model/requirements.txt (line 133))\n",
      "  Downloading python-etcd-0.4.5.tar.gz (37 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting python-json-logger==2.0.7 (from -r Model/image-classification-model/requirements.txt (line 134))\n",
      "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
      "Collecting pytz==2022.7 (from -r Model/image-classification-model/requirements.txt (line 135))\n",
      "  Downloading pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m499.4/499.4 kB\u001B[0m \u001B[31m54.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting PyWavelets==1.4.1 (from -r Model/image-classification-model/requirements.txt (line 136))\n",
      "  Downloading PyWavelets-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.8/6.8 MB\u001B[0m \u001B[31m80.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting PyYAML==6.0 (from -r Model/image-classification-model/requirements.txt (line 137))\n",
      "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m682.2/682.2 kB\u001B[0m \u001B[31m62.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pyzmq==25.1.0 (from -r Model/image-classification-model/requirements.txt (line 138))\n",
      "  Downloading pyzmq-25.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m53.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: qudida==0.0.4 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 139)) (0.0.4)\n",
      "Collecting referencing==0.30.0 (from -r Model/image-classification-model/requirements.txt (line 140))\n",
      "  Downloading referencing-0.30.0-py3-none-any.whl (25 kB)\n",
      "Collecting regex==2023.6.3 (from -r Model/image-classification-model/requirements.txt (line 141))\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m770.4/770.4 kB\u001B[0m \u001B[31m55.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting requests==2.29.0 (from -r Model/image-classification-model/requirements.txt (line 142))\n",
      "  Downloading requests-2.29.0-py3-none-any.whl (62 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.5/62.5 kB\u001B[0m \u001B[31m9.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: requests-oauthlib==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 143)) (1.3.1)\n",
      "Collecting rfc3339-validator==0.1.4 (from -r Model/image-classification-model/requirements.txt (line 144))\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting rfc3986==1.5.0 (from -r Model/image-classification-model/requirements.txt (line 145))\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting rfc3986-validator==0.1.1 (from -r Model/image-classification-model/requirements.txt (line 146))\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting rpds-py==0.9.2 (from -r Model/image-classification-model/requirements.txt (line 147))\n",
      "  Downloading rpds_py-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m62.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: rsa==4.9 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 148)) (4.9)\n",
      "Collecting ruamel.yaml==0.17.21 (from -r Model/image-classification-model/requirements.txt (line 149))\n",
      "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m109.5/109.5 kB\u001B[0m \u001B[31m11.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting ruamel.yaml.clib==0.2.6 (from -r Model/image-classification-model/requirements.txt (line 150))\n",
      "  Downloading ruamel.yaml.clib-0.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (519 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m519.3/519.3 kB\u001B[0m \u001B[31m44.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting safetensors==0.3.1 (from -r Model/image-classification-model/requirements.txt (line 151))\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m83.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting scikit-image==0.21.0 (from -r Model/image-classification-model/requirements.txt (line 152))\n",
      "  Downloading scikit_image-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.8/13.8 MB\u001B[0m \u001B[31m80.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting scikit-learn==1.3.0 (from -r Model/image-classification-model/requirements.txt (line 153))\n",
      "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.8/10.8 MB\u001B[0m \u001B[31m91.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting scipy==1.11.1 (from -r Model/image-classification-model/requirements.txt (line 154))\n",
      "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m36.3/36.3 MB\u001B[0m \u001B[31m12.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting Send2Trash==1.8.2 (from -r Model/image-classification-model/requirements.txt (line 155))\n",
      "  Downloading Send2Trash-1.8.2-py3-none-any.whl (18 kB)\n",
      "Collecting setuptools==65.6.3 (from -r Model/image-classification-model/requirements.txt (line 156))\n",
      "  Downloading setuptools-65.6.3-py3-none-any.whl (1.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m77.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 157)) (1.16.0)\n",
      "Collecting smart-open==6.3.0 (from -r Model/image-classification-model/requirements.txt (line 158))\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.8/56.8 kB\u001B[0m \u001B[31m7.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting sniffio==1.3.0 (from -r Model/image-classification-model/requirements.txt (line 159))\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 160)) (2.4.0)\n",
      "Collecting soupsieve==2.4 (from -r Model/image-classification-model/requirements.txt (line 161))\n",
      "  Downloading soupsieve-2.4-py3-none-any.whl (37 kB)\n",
      "Collecting stack-data==0.2.0 (from -r Model/image-classification-model/requirements.txt (line 162))\n",
      "  Downloading stack_data-0.2.0-py3-none-any.whl (21 kB)\n",
      "Collecting sympy==1.12 (from -r Model/image-classification-model/requirements.txt (line 163))\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.7/5.7 MB\u001B[0m \u001B[31m112.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tensorboard==2.13.0 (from -r Model/image-classification-model/requirements.txt (line 164))\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.6/5.6 MB\u001B[0m \u001B[31m111.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tensorboard-data-server==0.7.1 (from -r Model/image-classification-model/requirements.txt (line 165))\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.6/6.6 MB\u001B[0m \u001B[31m103.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting terminado==0.17.1 (from -r Model/image-classification-model/requirements.txt (line 166))\n",
      "  Downloading terminado-0.17.1-py3-none-any.whl (17 kB)\n",
      "Collecting textaugment==1.3.4 (from -r Model/image-classification-model/requirements.txt (line 167))\n",
      "  Downloading textaugment-1.3.4-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: textblob==0.17.1 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 168)) (0.17.1)\n",
      "Collecting threadpoolctl==3.2.0 (from -r Model/image-classification-model/requirements.txt (line 169))\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Collecting tifffile==2023.7.18 (from -r Model/image-classification-model/requirements.txt (line 170))\n",
      "  Downloading tifffile-2023.7.18-py3-none-any.whl (221 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m221.4/221.4 kB\u001B[0m \u001B[31m29.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting timm==0.9.2 (from -r Model/image-classification-model/requirements.txt (line 171))\n",
      "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m95.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tinycss2==1.2.1 (from -r Model/image-classification-model/requirements.txt (line 172))\n",
      "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting tokenizers==0.13.3 (from -r Model/image-classification-model/requirements.txt (line 173))\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.8/7.8 MB\u001B[0m \u001B[31m121.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tomli==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 174)) (2.0.1)\n",
      "Collecting toolz==0.12.0 (from -r Model/image-classification-model/requirements.txt (line 175))\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.8/55.8 kB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting torch==2.0.1 (from -r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m619.9/619.9 MB\u001B[0m \u001B[31m1.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting torchaudio==2.0.2 (from -r Model/image-classification-model/requirements.txt (line 177))\n",
      "  Downloading torchaudio-2.0.2-cp310-cp310-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.4/4.4 MB\u001B[0m \u001B[31m108.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting torchdata==0.6.1 (from -r Model/image-classification-model/requirements.txt (line 178))\n",
      "  Downloading torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.6/4.6 MB\u001B[0m \u001B[31m109.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting torchelastic==0.2.2 (from -r Model/image-classification-model/requirements.txt (line 179))\n",
      "  Downloading torchelastic-0.2.2-py3-none-any.whl (111 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m111.5/111.5 kB\u001B[0m \u001B[31m15.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting torchtext==0.15.2 (from -r Model/image-classification-model/requirements.txt (line 180))\n",
      "  Downloading torchtext-0.15.2-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m92.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting torchvision==0.15.2 (from -r Model/image-classification-model/requirements.txt (line 181))\n",
      "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.0/6.0 MB\u001B[0m \u001B[31m116.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tornado==6.3.2 (from -r Model/image-classification-model/requirements.txt (line 182))\n",
      "  Downloading tornado-6.3.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m426.9/426.9 kB\u001B[0m \u001B[31m48.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tqdm==4.65.0 (from -r Model/image-classification-model/requirements.txt (line 183))\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m77.1/77.1 kB\u001B[0m \u001B[31m11.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: traitlets==5.7.1 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 184)) (5.7.1)\n",
      "Collecting transformers==4.26.1 (from -r Model/image-classification-model/requirements.txt (line 185))\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m110.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting types-dataclasses==0.6.6 (from -r Model/image-classification-model/requirements.txt (line 186))\n",
      "  Downloading types_dataclasses-0.6.6-py3-none-any.whl (2.9 kB)\n",
      "Collecting typing_extensions==4.5.0 (from -r Model/image-classification-model/requirements.txt (line 187))\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting tzdata==2023.3 (from -r Model/image-classification-model/requirements.txt (line 188))\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m341.8/341.8 kB\u001B[0m \u001B[31m40.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting uri-template==1.3.0 (from -r Model/image-classification-model/requirements.txt (line 189))\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Collecting urllib3==1.26.15 (from -r Model/image-classification-model/requirements.txt (line 190))\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m140.9/140.9 kB\u001B[0m \u001B[31m21.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting virtualenv==20.24.1 (from -r Model/image-classification-model/requirements.txt (line 191))\n",
      "  Downloading virtualenv-20.24.1-py3-none-any.whl (3.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m100.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting wcwidth==0.2.5 (from -r Model/image-classification-model/requirements.txt (line 192))\n",
      "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
      "Collecting webcolors==1.13 (from -r Model/image-classification-model/requirements.txt (line 193))\n",
      "  Downloading webcolors-1.13-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.10/dist-packages (from -r Model/image-classification-model/requirements.txt (line 194)) (0.5.1)\n",
      "Collecting websocket-client==1.6.1 (from -r Model/image-classification-model/requirements.txt (line 195))\n",
      "  Downloading websocket_client-1.6.1-py3-none-any.whl (56 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.9/56.9 kB\u001B[0m \u001B[31m7.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting Werkzeug==2.3.6 (from -r Model/image-classification-model/requirements.txt (line 196))\n",
      "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m242.5/242.5 kB\u001B[0m \u001B[31m29.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting wheel==0.38.4 (from -r Model/image-classification-model/requirements.txt (line 197))\n",
      "  Downloading wheel-0.38.4-py3-none-any.whl (36 kB)\n",
      "Collecting zstandard==0.19.0 (from -r Model/image-classification-model/requirements.txt (line 198))\n",
      "  Downloading zstandard-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m69.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: jsonschema[format-nongpl]>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-events==0.6.3->-r Model/image-classification-model/requirements.txt (line 76)) (4.19.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.0/21.0 MB\u001B[0m \u001B[31m84.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m849.3/849.3 kB\u001B[0m \u001B[31m63.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.8/11.8 MB\u001B[0m \u001B[31m111.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m557.1/557.1 MB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m317.1/317.1 MB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m168.4/168.4 MB\u001B[0m \u001B[31m6.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m54.6/54.6 MB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m102.6/102.6 MB\u001B[0m \u001B[31m9.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m173.2/173.2 MB\u001B[0m \u001B[31m6.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m177.1/177.1 MB\u001B[0m \u001B[31m6.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m98.6/98.6 kB\u001B[0m \u001B[31m13.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting triton==2.0.0 (from torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m63.3/63.3 MB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176)) (3.27.9)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.1->-r Model/image-classification-model/requirements.txt (line 176))\n",
      "  Downloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m96.4/96.4 kB\u001B[0m \u001B[31m13.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hINFO: pip is looking at multiple versions of jsonschema[format-nongpl] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jsonschema[format-nongpl]>=3.2.0 (from jupyter-events==0.6.3->-r Model/image-classification-model/requirements.txt (line 76))\n",
      "  Downloading jsonschema-4.22.0-py3-none-any.whl (88 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m88.3/88.3 kB\u001B[0m \u001B[31m10.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Downloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.5/85.5 kB\u001B[0m \u001B[31m12.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Downloading jsonschema-4.21.0-py3-none-any.whl (85 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.1/85.1 kB\u001B[0m \u001B[31m12.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Downloading jsonschema-4.20.0-py3-none-any.whl (84 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.7/84.7 kB\u001B[0m \u001B[31m12.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Downloading jsonschema-4.19.1-py3-none-any.whl (83 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m83.3/83.3 kB\u001B[0m \u001B[31m11.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Downloading jsonschema-4.19.0-py3-none-any.whl (83 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m83.4/83.4 kB\u001B[0m \u001B[31m11.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Downloading jsonschema-4.18.6-py3-none-any.whl (83 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m83.2/83.2 kB\u001B[0m \u001B[31m11.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hINFO: pip is looking at multiple versions of jsonschema[format-nongpl] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading jsonschema-4.18.5-py3-none-any.whl (82 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m82.5/82.5 kB\u001B[0m \u001B[31m11.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[33mWARNING: The candidate selected for download or install is a yanked version: 'opencv-python-headless' candidate (version 4.8.0.74 at https://files.pythonhosted.org/packages/76/02/f128517f3ade4bb5f71e2afd8461dba70e3f466ce745fa1fd1fade9ad1b7/opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from https://pypi.org/simple/opencv-python-headless/) (requires-python:>=3.6))\n",
      "Reason for being yanked: deprecated, use 4.8.0.76\u001B[0m\u001B[33m\n",
      "\u001B[0mBuilding wheels for collected packages: antlr4-python3-runtime, googletrans, pycosat, python-etcd\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=d7bfb290c7c2053afaf1043c0ee2e73ff90414e6bc3bbe88fab65846c6a5aebc\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "  Building wheel for googletrans (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15718 sha256=87d012e28936c3897020c8b5096e66dd1e0545c4d62eea289e5a2f1e59ceba57\n",
      "  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n",
      "  Building wheel for pycosat (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pycosat: filename=pycosat-0.6.6-cp310-cp310-linux_x86_64.whl size=169338 sha256=8b7072534441224c89d0445bef1cf15a1f436b62eddeb8942e5fe639779d64e9\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/29/df/b8c22ca5812e2d7b342269a53add280b5bad42a540f34c3dc1\n",
      "  Building wheel for python-etcd (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for python-etcd: filename=python_etcd-0.4.5-py3-none-any.whl size=38481 sha256=2cb14a7ddf34096c2a85a6c63d0374790c44f1a72a808048e62cc79a2ef685c2\n",
      "  Stored in directory: /root/.cache/pip/wheels/93/5f/1b/056db07a0ab1c0b7efe175928d2a10b614e0e00d7bab0b6496\n",
      "Successfully built antlr4-python3-runtime googletrans pycosat python-etcd\n",
      "Installing collected packages: wcwidth, types-dataclasses, tokenizers, safetensors, rfc3986, pytz, pycosat, pure-eval, lit, libarchive-c, json5, hyperframe, hpack, h11, gmpy2, fastjsonschema, executing, distlib, chardet, boltons, antlr4-python3-runtime, zstandard, wheel, websocket-client, webcolors, urllib3, uri-template, tzdata, typing_extensions, tqdm, tornado, toolz, tinycss2, threadpoolctl, tensorboard-data-server, sympy, soupsieve, sniffio, smart-open, setuptools, Send2Trash, ruamel.yaml.clib, rpds-py, rfc3986-validator, rfc3339-validator, regex, pyzmq, PyYAML, python-json-logger, pyparsing, pynvml, Pygments, pycparser, pyasn1, psutil, protobuf, prompt-toolkit, prometheus-client, pluggy, platformdirs, pkginfo, pip, pexpect, parso, pandocfilters, packaging, overrides, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, numpy, networkx, nest-asyncio, mistune, matplotlib-inline, MarkupSafe, Markdown, lazy_loader, kiwisolver, jupyterlab-pygments, jsonpointer, joblib, idna, identify, hstspreload, h2, grpcio, fqdn, fonttools, filelock, expecttest, exceptiongroup, einops, dnspython, decorator, debugpy, cycler, comm, click, charset-normalizer, cfgv, certifi, cachetools, bleach, Babel, attrs, asttokens, Werkzeug, virtualenv, tifffile, terminado, stack-data, scipy, ruamel.yaml, requests, referencing, PyWavelets, python-etcd, pyasn1-modules, opencv-python-headless, omegaconf, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nodeenv, jupyter_core, jsonpatch, Jinja2, jedi, imageio, hypothesis, httpcore, contourpy, cffi, beautifulsoup4, async-lru, arrow, scikit-learn, scikit-image, pre-commit, nvidia-cusolver-cu11, nvidia-cudnn-cu11, matplotlib, jupyter_server_terminals, jupyter_client, jsonschema-specifications, isoduration, ipython, hydra-core, huggingface-hub, httpx, google-auth, gensim, cryptography, brotlipy, transformers, pyOpenSSL, jsonschema, ipykernel, googletrans, google-auth-oauthlib, argon2-cffi, textaugment, tensorboard, nbformat, nbclient, jupyter-events, nbconvert, jupyter_server, notebook_shim, jupyterlab_server, jupyter-lsp, jupyterlab, triton, torch, torchvision, torchdata, torchtext, torchelastic, torchaudio, timm\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.13\n",
      "    Uninstalling wcwidth-0.2.13:\n",
      "      Successfully uninstalled wcwidth-0.2.13\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.3\n",
      "    Uninstalling safetensors-0.4.3:\n",
      "      Successfully uninstalled safetensors-0.4.3\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2023.4\n",
      "    Uninstalling pytz-2023.4:\n",
      "      Successfully uninstalled pytz-2023.4\n",
      "  Attempting uninstall: fastjsonschema\n",
      "    Found existing installation: fastjsonschema 2.20.0\n",
      "    Uninstalling fastjsonschema-2.20.0:\n",
      "      Successfully uninstalled fastjsonschema-2.20.0\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 5.2.0\n",
      "    Uninstalling chardet-5.2.0:\n",
      "      Successfully uninstalled chardet-5.2.0\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.43.0\n",
      "    Uninstalling wheel-0.43.0:\n",
      "      Successfully uninstalled wheel-0.43.0\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 1.8.0\n",
      "    Uninstalling websocket-client-1.8.0:\n",
      "      Successfully uninstalled websocket-client-1.8.0\n",
      "  Attempting uninstall: webcolors\n",
      "    Found existing installation: webcolors 24.6.0\n",
      "    Uninstalling webcolors-24.6.0:\n",
      "      Successfully uninstalled webcolors-24.6.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2024.1\n",
      "    Uninstalling tzdata-2024.1:\n",
      "      Successfully uninstalled tzdata-2024.1\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.4\n",
      "    Uninstalling tqdm-4.66.4:\n",
      "      Successfully uninstalled tqdm-4.66.4\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.3.3\n",
      "    Uninstalling tornado-6.3.3:\n",
      "      Successfully uninstalled tornado-6.3.3\n",
      "  Attempting uninstall: toolz\n",
      "    Found existing installation: toolz 0.12.1\n",
      "    Uninstalling toolz-0.12.1:\n",
      "      Successfully uninstalled toolz-0.12.1\n",
      "  Attempting uninstall: tinycss2\n",
      "    Found existing installation: tinycss2 1.3.0\n",
      "    Uninstalling tinycss2-1.3.0:\n",
      "      Successfully uninstalled tinycss2-1.3.0\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.5.0\n",
      "    Uninstalling threadpoolctl-3.5.0:\n",
      "      Successfully uninstalled threadpoolctl-3.5.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12.1\n",
      "    Uninstalling sympy-1.12.1:\n",
      "      Successfully uninstalled sympy-1.12.1\n",
      "  Attempting uninstall: soupsieve\n",
      "    Found existing installation: soupsieve 2.5\n",
      "    Uninstalling soupsieve-2.5:\n",
      "      Successfully uninstalled soupsieve-2.5\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.1\n",
      "    Uninstalling sniffio-1.3.1:\n",
      "      Successfully uninstalled sniffio-1.3.1\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 7.0.4\n",
      "    Uninstalling smart-open-7.0.4:\n",
      "      Successfully uninstalled smart-open-7.0.4\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 67.7.2\n",
      "    Uninstalling setuptools-67.7.2:\n",
      "      Successfully uninstalled setuptools-67.7.2\n",
      "  Attempting uninstall: Send2Trash\n",
      "    Found existing installation: Send2Trash 1.8.3\n",
      "    Uninstalling Send2Trash-1.8.3:\n",
      "      Successfully uninstalled Send2Trash-1.8.3\n",
      "  Attempting uninstall: rpds-py\n",
      "    Found existing installation: rpds-py 0.18.1\n",
      "    Uninstalling rpds-py-0.18.1:\n",
      "      Successfully uninstalled rpds-py-0.18.1\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.5.15\n",
      "    Uninstalling regex-2024.5.15:\n",
      "      Successfully uninstalled regex-2024.5.15\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 24.0.1\n",
      "    Uninstalling pyzmq-24.0.1:\n",
      "      Successfully uninstalled pyzmq-24.0.1\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.2\n",
      "    Uninstalling pyparsing-3.1.2:\n",
      "      Successfully uninstalled pyparsing-3.1.2\n",
      "  Attempting uninstall: Pygments\n",
      "    Found existing installation: Pygments 2.16.1\n",
      "    Uninstalling Pygments-2.16.1:\n",
      "      Successfully uninstalled Pygments-2.16.1\n",
      "  Attempting uninstall: pycparser\n",
      "    Found existing installation: pycparser 2.22\n",
      "    Uninstalling pycparser-2.22:\n",
      "      Successfully uninstalled pycparser-2.22\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.6.0\n",
      "    Uninstalling pyasn1-0.6.0:\n",
      "      Successfully uninstalled pyasn1-0.6.0\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.5\n",
      "    Uninstalling psutil-5.9.5:\n",
      "      Successfully uninstalled psutil-5.9.5\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt_toolkit 3.0.47\n",
      "    Uninstalling prompt_toolkit-3.0.47:\n",
      "      Successfully uninstalled prompt_toolkit-3.0.47\n",
      "  Attempting uninstall: prometheus-client\n",
      "    Found existing installation: prometheus_client 0.20.0\n",
      "    Uninstalling prometheus_client-0.20.0:\n",
      "      Successfully uninstalled prometheus_client-0.20.0\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 1.5.0\n",
      "    Uninstalling pluggy-1.5.0:\n",
      "      Successfully uninstalled pluggy-1.5.0\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 4.2.2\n",
      "    Uninstalling platformdirs-4.2.2:\n",
      "      Successfully uninstalled platformdirs-4.2.2\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.1.2\n",
      "    Uninstalling pip-23.1.2:\n",
      "      Successfully uninstalled pip-23.1.2\n",
      "  Attempting uninstall: pexpect\n",
      "    Found existing installation: pexpect 4.9.0\n",
      "    Uninstalling pexpect-4.9.0:\n",
      "      Successfully uninstalled pexpect-4.9.0\n",
      "  Attempting uninstall: parso\n",
      "    Found existing installation: parso 0.8.4\n",
      "    Uninstalling parso-0.8.4:\n",
      "      Successfully uninstalled parso-0.8.4\n",
      "  Attempting uninstall: pandocfilters\n",
      "    Found existing installation: pandocfilters 1.5.1\n",
      "    Uninstalling pandocfilters-1.5.1:\n",
      "      Successfully uninstalled pandocfilters-1.5.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.3\n",
      "    Uninstalling networkx-3.3:\n",
      "      Successfully uninstalled networkx-3.3\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest-asyncio 1.6.0\n",
      "    Uninstalling nest-asyncio-1.6.0:\n",
      "      Successfully uninstalled nest-asyncio-1.6.0\n",
      "  Attempting uninstall: mistune\n",
      "    Found existing installation: mistune 0.8.4\n",
      "    Uninstalling mistune-0.8.4:\n",
      "      Successfully uninstalled mistune-0.8.4\n",
      "  Attempting uninstall: matplotlib-inline\n",
      "    Found existing installation: matplotlib-inline 0.1.7\n",
      "    Uninstalling matplotlib-inline-0.1.7:\n",
      "      Successfully uninstalled matplotlib-inline-0.1.7\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.5\n",
      "    Uninstalling MarkupSafe-2.1.5:\n",
      "      Successfully uninstalled MarkupSafe-2.1.5\n",
      "  Attempting uninstall: Markdown\n",
      "    Found existing installation: Markdown 3.6\n",
      "    Uninstalling Markdown-3.6:\n",
      "      Successfully uninstalled Markdown-3.6\n",
      "  Attempting uninstall: lazy_loader\n",
      "    Found existing installation: lazy_loader 0.4\n",
      "    Uninstalling lazy_loader-0.4:\n",
      "      Successfully uninstalled lazy_loader-0.4\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.4.5\n",
      "    Uninstalling kiwisolver-1.4.5:\n",
      "      Successfully uninstalled kiwisolver-1.4.5\n",
      "  Attempting uninstall: jupyterlab-pygments\n",
      "    Found existing installation: jupyterlab_pygments 0.3.0\n",
      "    Uninstalling jupyterlab_pygments-0.3.0:\n",
      "      Successfully uninstalled jupyterlab_pygments-0.3.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.4.2\n",
      "    Uninstalling joblib-1.4.2:\n",
      "      Successfully uninstalled joblib-1.4.2\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.7\n",
      "    Uninstalling idna-3.7:\n",
      "      Successfully uninstalled idna-3.7\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.64.1\n",
      "    Uninstalling grpcio-1.64.1:\n",
      "      Successfully uninstalled grpcio-1.64.1\n",
      "  Attempting uninstall: fonttools\n",
      "    Found existing installation: fonttools 4.53.0\n",
      "    Uninstalling fonttools-4.53.0:\n",
      "      Successfully uninstalled fonttools-4.53.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.15.4\n",
      "    Uninstalling filelock-3.15.4:\n",
      "      Successfully uninstalled filelock-3.15.4\n",
      "  Attempting uninstall: exceptiongroup\n",
      "    Found existing installation: exceptiongroup 1.2.1\n",
      "    Uninstalling exceptiongroup-1.2.1:\n",
      "      Successfully uninstalled exceptiongroup-1.2.1\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 4.4.2\n",
      "    Uninstalling decorator-4.4.2:\n",
      "      Successfully uninstalled decorator-4.4.2\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.6.6\n",
      "    Uninstalling debugpy-1.6.6:\n",
      "      Successfully uninstalled debugpy-1.6.6\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.12.1\n",
      "    Uninstalling cycler-0.12.1:\n",
      "      Successfully uninstalled cycler-0.12.1\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.7\n",
      "    Uninstalling click-8.1.7:\n",
      "      Successfully uninstalled click-8.1.7\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.6.2\n",
      "    Uninstalling certifi-2024.6.2:\n",
      "      Successfully uninstalled certifi-2024.6.2\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.3.3\n",
      "    Uninstalling cachetools-5.3.3:\n",
      "      Successfully uninstalled cachetools-5.3.3\n",
      "  Attempting uninstall: bleach\n",
      "    Found existing installation: bleach 6.1.0\n",
      "    Uninstalling bleach-6.1.0:\n",
      "      Successfully uninstalled bleach-6.1.0\n",
      "  Attempting uninstall: Babel\n",
      "    Found existing installation: Babel 2.15.0\n",
      "    Uninstalling Babel-2.15.0:\n",
      "      Successfully uninstalled Babel-2.15.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.2.0\n",
      "    Uninstalling attrs-23.2.0:\n",
      "      Successfully uninstalled attrs-23.2.0\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 3.0.3\n",
      "    Uninstalling Werkzeug-3.0.3:\n",
      "      Successfully uninstalled Werkzeug-3.0.3\n",
      "  Attempting uninstall: tifffile\n",
      "    Found existing installation: tifffile 2024.6.18\n",
      "    Uninstalling tifffile-2024.6.18:\n",
      "      Successfully uninstalled tifffile-2024.6.18\n",
      "  Attempting uninstall: terminado\n",
      "    Found existing installation: terminado 0.18.1\n",
      "    Uninstalling terminado-0.18.1:\n",
      "      Successfully uninstalled terminado-0.18.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.4\n",
      "    Uninstalling scipy-1.11.4:\n",
      "      Successfully uninstalled scipy-1.11.4\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: referencing\n",
      "    Found existing installation: referencing 0.35.1\n",
      "    Uninstalling referencing-0.35.1:\n",
      "      Successfully uninstalled referencing-0.35.1\n",
      "  Attempting uninstall: PyWavelets\n",
      "    Found existing installation: PyWavelets 1.6.0\n",
      "    Uninstalling PyWavelets-1.6.0:\n",
      "      Successfully uninstalled PyWavelets-1.6.0\n",
      "  Attempting uninstall: pyasn1-modules\n",
      "    Found existing installation: pyasn1_modules 0.4.0\n",
      "    Uninstalling pyasn1_modules-0.4.0:\n",
      "      Successfully uninstalled pyasn1_modules-0.4.0\n",
      "  Attempting uninstall: opencv-python-headless\n",
      "    Found existing installation: opencv-python-headless 4.10.0.84\n",
      "    Uninstalling opencv-python-headless-4.10.0.84:\n",
      "      Successfully uninstalled opencv-python-headless-4.10.0.84\n",
      "  Attempting uninstall: jupyter_core\n",
      "    Found existing installation: jupyter_core 5.7.2\n",
      "    Uninstalling jupyter_core-5.7.2:\n",
      "      Successfully uninstalled jupyter_core-5.7.2\n",
      "  Attempting uninstall: Jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: imageio\n",
      "    Found existing installation: imageio 2.31.6\n",
      "    Uninstalling imageio-2.31.6:\n",
      "      Successfully uninstalled imageio-2.31.6\n",
      "  Attempting uninstall: contourpy\n",
      "    Found existing installation: contourpy 1.2.1\n",
      "    Uninstalling contourpy-1.2.1:\n",
      "      Successfully uninstalled contourpy-1.2.1\n",
      "  Attempting uninstall: cffi\n",
      "    Found existing installation: cffi 1.16.0\n",
      "    Uninstalling cffi-1.16.0:\n",
      "      Successfully uninstalled cffi-1.16.0\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.12.3\n",
      "    Uninstalling beautifulsoup4-4.12.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.12.3\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "  Attempting uninstall: scikit-image\n",
      "    Found existing installation: scikit-image 0.19.3\n",
      "    Uninstalling scikit-image-0.19.3:\n",
      "      Successfully uninstalled scikit-image-0.19.3\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.1\n",
      "    Uninstalling matplotlib-3.7.1:\n",
      "      Successfully uninstalled matplotlib-3.7.1\n",
      "  Attempting uninstall: jupyter_client\n",
      "    Found existing installation: jupyter-client 6.1.12\n",
      "    Uninstalling jupyter-client-6.1.12:\n",
      "      Successfully uninstalled jupyter-client-6.1.12\n",
      "  Attempting uninstall: jsonschema-specifications\n",
      "    Found existing installation: jsonschema-specifications 2023.12.1\n",
      "    Uninstalling jsonschema-specifications-2023.12.1:\n",
      "      Successfully uninstalled jsonschema-specifications-2023.12.1\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 7.34.0\n",
      "    Uninstalling ipython-7.34.0:\n",
      "      Successfully uninstalled ipython-7.34.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.23.4\n",
      "    Uninstalling huggingface-hub-0.23.4:\n",
      "      Successfully uninstalled huggingface-hub-0.23.4\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.27.0\n",
      "    Uninstalling google-auth-2.27.0:\n",
      "      Successfully uninstalled google-auth-2.27.0\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.3.2\n",
      "    Uninstalling gensim-4.3.2:\n",
      "      Successfully uninstalled gensim-4.3.2\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 42.0.8\n",
      "    Uninstalling cryptography-42.0.8:\n",
      "      Successfully uninstalled cryptography-42.0.8\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.41.2\n",
      "    Uninstalling transformers-4.41.2:\n",
      "      Successfully uninstalled transformers-4.41.2\n",
      "  Attempting uninstall: pyOpenSSL\n",
      "    Found existing installation: pyOpenSSL 24.1.0\n",
      "    Uninstalling pyOpenSSL-24.1.0:\n",
      "      Successfully uninstalled pyOpenSSL-24.1.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.19.2\n",
      "    Uninstalling jsonschema-4.19.2:\n",
      "      Successfully uninstalled jsonschema-4.19.2\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 5.5.6\n",
      "    Uninstalling ipykernel-5.5.6:\n",
      "      Successfully uninstalled ipykernel-5.5.6\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.2.0\n",
      "    Uninstalling google-auth-oauthlib-1.2.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
      "  Attempting uninstall: argon2-cffi\n",
      "    Found existing installation: argon2-cffi 23.1.0\n",
      "    Uninstalling argon2-cffi-23.1.0:\n",
      "      Successfully uninstalled argon2-cffi-23.1.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.15.2\n",
      "    Uninstalling tensorboard-2.15.2:\n",
      "      Successfully uninstalled tensorboard-2.15.2\n",
      "  Attempting uninstall: nbformat\n",
      "    Found existing installation: nbformat 5.10.4\n",
      "    Uninstalling nbformat-5.10.4:\n",
      "      Successfully uninstalled nbformat-5.10.4\n",
      "  Attempting uninstall: nbclient\n",
      "    Found existing installation: nbclient 0.10.0\n",
      "    Uninstalling nbclient-0.10.0:\n",
      "      Successfully uninstalled nbclient-0.10.0\n",
      "  Attempting uninstall: nbconvert\n",
      "    Found existing installation: nbconvert 6.5.4\n",
      "    Uninstalling nbconvert-6.5.4:\n",
      "      Successfully uninstalled nbconvert-6.5.4\n",
      "  Attempting uninstall: jupyter_server\n",
      "    Found existing installation: jupyter-server 1.24.0\n",
      "    Uninstalling jupyter-server-1.24.0:\n",
      "      Successfully uninstalled jupyter-server-1.24.0\n",
      "  Attempting uninstall: notebook_shim\n",
      "    Found existing installation: notebook_shim 0.2.4\n",
      "    Uninstalling notebook_shim-0.2.4:\n",
      "      Successfully uninstalled notebook_shim-0.2.4\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.3.0\n",
      "    Uninstalling triton-2.3.0:\n",
      "      Successfully uninstalled triton-2.3.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.0+cu121\n",
      "    Uninstalling torch-2.3.0+cu121:\n",
      "      Successfully uninstalled torch-2.3.0+cu121\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.18.0+cu121\n",
      "    Uninstalling torchvision-0.18.0+cu121:\n",
      "      Successfully uninstalled torchvision-0.18.0+cu121\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.18.0\n",
      "    Uninstalling torchtext-0.18.0:\n",
      "      Successfully uninstalled torchtext-0.18.0\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.3.0+cu121\n",
      "    Uninstalling torchaudio-2.3.0+cu121:\n",
      "      Successfully uninstalled torchaudio-2.3.0+cu121\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sqlalchemy 2.0.31 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.27.0, but you have google-auth 2.22.0 which is incompatible.\n",
      "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.24.0 which is incompatible.\n",
      "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 8.12.0 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.29.0 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.3.3, but you have tornado 6.3.2 which is incompatible.\n",
      "kaggle 1.6.14 requires certifi>=2023.7.22, but you have certifi 2023.5.7 which is incompatible.\n",
      "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
      "notebook 6.5.5 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.3.0 which is incompatible.\n",
      "notebook 6.5.5 requires pyzmq<25,>=17, but you have pyzmq 25.1.0 which is incompatible.\n",
      "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\n",
      "pydantic 2.7.4 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.18.4 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "tensorflow 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.13.0 which is incompatible.\n",
      "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.23.4 which is incompatible.\n",
      "yfinance 0.2.40 requires requests>=2.31, but you have requests 2.29.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed Babel-2.12.1 Jinja2-3.1.2 Markdown-3.4.3 MarkupSafe-2.1.1 PyWavelets-1.4.1 PyYAML-6.0 Pygments-2.15.1 Send2Trash-1.8.2 Werkzeug-2.3.6 antlr4-python3-runtime-4.9.3 argon2-cffi-21.3.0 arrow-1.2.3 asttokens-2.0.5 async-lru-2.0.3 attrs-23.1.0 beautifulsoup4-4.12.2 bleach-6.0.0 boltons-23.0.0 brotlipy-0.7.0 cachetools-5.3.1 certifi-2023.5.7 cffi-1.15.1 cfgv-3.3.1 chardet-3.0.4 charset-normalizer-2.0.4 click-8.1.6 comm-0.1.3 contourpy-1.1.0 cryptography-39.0.1 cycler-0.11.0 debugpy-1.6.7 decorator-5.1.1 distlib-0.3.7 dnspython-2.3.0 einops-0.6.1 exceptiongroup-1.1.1 executing-0.8.3 expecttest-0.1.4 fastjsonschema-2.17.1 filelock-3.12.2 fonttools-4.41.1 fqdn-1.5.1 gensim-4.3.1 gmpy2-2.1.2 google-auth-2.22.0 google-auth-oauthlib-1.0.0 googletrans-3.0.0 grpcio-1.56.2 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 huggingface-hub-0.16.4 hydra-core-1.3.2 hyperframe-5.2.0 hypothesis-6.75.2 identify-2.5.26 idna-2.10 imageio-2.31.1 ipykernel-6.24.0 ipython-8.12.0 isoduration-20.11.0 jedi-0.18.1 joblib-1.3.1 json5-0.9.14 jsonpatch-1.32 jsonpointer-2.1 jsonschema-4.18.4 jsonschema-specifications-2023.7.1 jupyter-events-0.6.3 jupyter-lsp-2.2.0 jupyter_client-8.3.0 jupyter_core-5.3.1 jupyter_server-2.7.0 jupyter_server_terminals-0.4.4 jupyterlab-4.0.3 jupyterlab-pygments-0.2.2 jupyterlab_server-2.23.0 kiwisolver-1.4.4 lazy_loader-0.3 libarchive-c-2.9 lit-18.1.8 matplotlib-3.7.2 matplotlib-inline-0.1.6 mistune-3.0.1 nbclient-0.8.0 nbconvert-7.7.2 nbformat-5.9.1 nest-asyncio-1.5.6 networkx-3.1 nodeenv-1.8.0 notebook_shim-0.2.3 numpy-1.24.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 omegaconf-2.3.0 opencv-python-headless-4.8.0.74 overrides-7.3.1 packaging-23.0 pandocfilters-1.5.0 parso-0.8.3 pexpect-4.8.0 pip-23.0.1 pkginfo-1.9.6 platformdirs-3.9.1 pluggy-1.0.0 pre-commit-3.3.3 prometheus-client-0.17.1 prompt-toolkit-3.0.36 protobuf-4.23.4 psutil-5.9.0 pure-eval-0.2.2 pyOpenSSL-23.0.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycosat-0.6.6 pycparser-2.21 pynvml-11.5.0 pyparsing-3.0.9 python-etcd-0.4.5 python-json-logger-2.0.7 pytz-2022.7 pyzmq-25.1.0 referencing-0.30.0 regex-2023.6.3 requests-2.29.0 rfc3339-validator-0.1.4 rfc3986-1.5.0 rfc3986-validator-0.1.1 rpds-py-0.9.2 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 safetensors-0.3.1 scikit-image-0.21.0 scikit-learn-1.3.0 scipy-1.11.1 setuptools-65.6.3 smart-open-6.3.0 sniffio-1.3.0 soupsieve-2.4 stack-data-0.2.0 sympy-1.12 tensorboard-2.13.0 tensorboard-data-server-0.7.1 terminado-0.17.1 textaugment-1.3.4 threadpoolctl-3.2.0 tifffile-2023.7.18 timm-0.9.2 tinycss2-1.2.1 tokenizers-0.13.3 toolz-0.12.0 torch-2.0.1 torchaudio-2.0.2 torchdata-0.6.1 torchelastic-0.2.2 torchtext-0.15.2 torchvision-0.15.2 tornado-6.3.2 tqdm-4.65.0 transformers-4.26.1 triton-2.0.0 types-dataclasses-0.6.6 typing_extensions-4.5.0 tzdata-2023.3 uri-template-1.3.0 urllib3-1.26.15 virtualenv-20.24.1 wcwidth-0.2.5 webcolors-1.13 websocket-client-1.6.1 wheel-0.38.4 zstandard-0.19.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "IPython",
         "_distutils_hack",
         "certifi",
         "cffi",
         "cycler",
         "debugpy",
         "decorator",
         "google",
         "kiwisolver",
         "matplotlib",
         "matplotlib_inline",
         "mpl_toolkits",
         "numpy",
         "pexpect",
         "pkg_resources",
         "prompt_toolkit",
         "psutil",
         "pydevd_plugins",
         "pygments",
         "setuptools",
         "tornado",
         "wcwidth",
         "zmq"
        ]
       },
       "id": "e7cf6a4a97df4574bb49d0c621e9f336"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "!pip install -r \"Model/image-classification-model/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075c892f-f35f-4193-b62d-d4528c0a9e85",
   "metadata": {
    "id": "075c892f-f35f-4193-b62d-d4528c0a9e85",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1719923098639,
     "user_tz": -60,
     "elapsed": 2,
     "user": {
      "displayName": "Jianfei Xu",
      "userId": "14747087902840301965"
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d739a7ad-9d68-40dd-a9b3-c1eb0fc55e06",
   "metadata": {
    "id": "d739a7ad-9d68-40dd-a9b3-c1eb0fc55e06"
   },
   "source": [
    "# 3.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a16609b4-8d93-4c11-9cfe-a8e21a81cf9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a16609b4-8d93-4c11-9cfe-a8e21a81cf9a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1719928938568,
     "user_tz": -60,
     "elapsed": 5828755,
     "user": {
      "displayName": "Jianfei Xu",
      "userId": "14747087902840301965"
     }
    },
    "outputId": "53c940d8-f887-400c-ff7d-5000c147f66f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2024-07-02 12:25:19.278362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-02 12:25:19.278423: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-02 12:25:19.390785: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-02 12:25:19.608067: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-02 12:25:21.904231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[2024-07-02 12:25:43,143][__main__][INFO] - Configurations:\n",
      "base:\n",
      "  seed: 1234\n",
      "  amp: true\n",
      "  image_size: 224\n",
      "  text_max_length: 256\n",
      "  loss_best: contrastive\n",
      "  data_frac: 1.0\n",
      "  output:\n",
      "    checkpoint: Model/image-classification-model/runs/backbone/outputs/2024-07-02/12-25-42/checkpoints/\n",
      "    tensorboard: Model/image-classification-model/runs/backbone/outputs/2024-07-02/12-25-42/tensorboard/\n",
      "data_train:\n",
      "  mimic_cxr:\n",
      "    name: mimic_cxr\n",
      "    data_type: imagetext\n",
      "    data_path: Model/image-classification-model/datasets/mimic-cxr-jpg/mimic_train.csv\n",
      "    image_size: 224\n",
      "    text_max_length: 256\n",
      "    data_frac: 1.0\n",
      "data_valid:\n",
      "  mimic_cxr:\n",
      "    name: mimic_cxr\n",
      "    data_type: imagetext\n",
      "    data_path: Model/image-classification-model/datasets/mimic-cxr-jpg/mimic_valid.csv\n",
      "    image_size: 224\n",
      "    text_max_length: 256\n",
      "dataloader:\n",
      "  train:\n",
      "    pin_memory: true\n",
      "    shuffle: true\n",
      "    drop_last: true\n",
      "    num_workers: 8\n",
      "    prefetch_factor: 8\n",
      "    batch_size: 32\n",
      "  valid:\n",
      "    pin_memory: true\n",
      "    shuffle: false\n",
      "    drop_last: false\n",
      "    num_workers: 8\n",
      "    prefetch_factor: 8\n",
      "    batch_size: 32\n",
      "  test:\n",
      "    pin_memory: true\n",
      "    shuffle: false\n",
      "    drop_last: false\n",
      "    num_workers: 16\n",
      "    batch_size: 32\n",
      "tokenizer:\n",
      "  source: huggingface\n",
      "  pretrained_model_name_or_path: emilyalsentzer/Bio_ClinicalBERT\n",
      "  cache_dir: huggingface/tokenizers\n",
      "transform:\n",
      "  train:\n",
      "    RandomResizedCrop:\n",
      "      size: 224\n",
      "      scale:\n",
      "      - 0.8\n",
      "      - 1.1\n",
      "    CLAHE:\n",
      "      clip_limit: 4.0\n",
      "    ColorJitter:\n",
      "      brightness: 0.1\n",
      "      contrast: 0.2\n",
      "      saturation: 0.2\n",
      "      hue: 0.1\n",
      "    CenterCrop:\n",
      "      size: 224\n",
      "  valid:\n",
      "    Resize:\n",
      "      size: 224\n",
      "    CenterCrop:\n",
      "      size: 224\n",
      "  test:\n",
      "    Resize:\n",
      "      size: 224\n",
      "    CenterCrop:\n",
      "      size: 224\n",
      "model:\n",
      "  name: clip_custom\n",
      "  temperature: 0.07\n",
      "  image_encoder:\n",
      "    source: torchvision\n",
      "    name: resnet\n",
      "    pretrained: true\n",
      "  text_encoder:\n",
      "    source: huggingface\n",
      "    name: emilyalsentzer/Bio_ClinicalBERT\n",
      "    pretrained: true\n",
      "    gradient_checkpointing: false\n",
      "    pooling: eos\n",
      "    cache_dir: huggingface/\n",
      "    trust_remote_code: true\n",
      "    mlm_head: true\n",
      "  projection_head:\n",
      "    name: linear\n",
      "    dropout: 0.1\n",
      "    proj_dim: 512\n",
      "optimizer:\n",
      "  name: adamw\n",
      "  config:\n",
      "    lr: 5.0e-05\n",
      "    weight_decay: 0.0001\n",
      "scheduler:\n",
      "  name: cosine\n",
      "  config:\n",
      "    total_epochs: 5\n",
      "    warmup_epochs: 1\n",
      "loss:\n",
      "  cxr_clip:\n",
      "    label_smoothing: 0.0\n",
      "    i2i_weight: 1.0\n",
      "    t2t_weight: 0.5\n",
      "    loss_ratio: 1.0\n",
      "\n",
      "[2024-07-02 12:25:43,143][cxrclip.util.utils][INFO] - Global seed set to 1234\n",
      "[2024-07-02 12:25:43,460][cxrclip.trainer][INFO] - DistEnv: DistEnv(world_size=1, world_rank=0, local_rank=0, num_gpus=1, master=True, summary_writer=<cxrclip.util.global_env.SummaryWriter object at 0x7b79aaaa7e20>)\n",
      "[2024-07-02 12:25:43,460][cxrclip.trainer][INFO] - cuda: Load datasets\n",
      "Downloading config.json: 100% 385/385 [00:00<00:00, 2.71MB/s]\n",
      "Downloading vocab.txt: 100% 213k/213k [00:00<00:00, 11.0MB/s]\n",
      "[2024-07-02 12:25:45,534][cxrclip.data.datamodule][INFO] - Dataset loaded: mimic_cxr for train\n",
      "[2024-07-02 12:25:45,936][cxrclip.data.datamodule][INFO] - Dataset loaded: mimic_cxr for valid\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "[2024-07-02 12:25:45,937][cxrclip.trainer][INFO] - cuda: Build the model\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100% 97.8M/97.8M [00:00<00:00, 309MB/s]\n",
      "Downloading pytorch_model.bin: 100% 436M/436M [00:03<00:00, 143MB/s]\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2024-07-02 12:25:53,112][cxrclip.trainer][INFO] - cuda: Model info:\n",
      "CXRClip(\n",
      "  (image_encoder): ResNet50(\n",
      "    (resnet): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (text_encoder): HuggingfaceTextEncoder(\n",
      "    (text_encoder): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (image_projection): LinearProjectionHead(\n",
      "    (projection): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (text_projection): LinearProjectionHead(\n",
      "    (projection): Linear(in_features=768, out_features=512, bias=True)\n",
      "  )\n",
      ")\n",
      "[2024-07-02 12:25:53,161][cxrclip.trainer][INFO] - cuda: Build the loss function\n",
      "[2024-07-02 12:25:53,161][cxrclip.trainer][INFO] - cuda: Build the optimizer\n",
      "[2024-07-02 12:25:53,163][cxrclip.trainer][INFO] - cuda: Build the LR scheulder\n",
      "[2024-07-02 12:25:53,163][cxrclip.trainer][INFO] - Download nltk module\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[2024-07-02 12:25:53,634][cxrclip.trainer][INFO] - cuda: Train the model\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "[000/005 epoch train]:   0% 0/166 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "[000/005 epoch train]: 100% 166/166 [20:36<00:00,  7.45s/it, lr=['0.00004548'], loss=4.325005, CUDA-Mem=58%, CUDA-Util=63%]\n",
      "[000/005 epoch valid]: 100% 2/2 [00:33<00:00, 16.77s/it, loss=6.288781, CUDA-Mem(%)=20, CUDA-Util(%)=29]\n",
      "[2024-07-02 12:47:11,574][cxrclip.trainer][INFO] - Epoch 0, last-model saved\n",
      "[2024-07-02 12:47:32,111][cxrclip.trainer][INFO] - Model/image-classification-model/runs/backbone/outputs/2024-07-02/12-25-42/checkpoints/model-best.tar saved\n",
      "[001/005 epoch train]: 100% 166/166 [18:22<00:00,  6.64s/it, lr=['0.00004389'], loss=3.806606, CUDA-Mem=83%, CUDA-Util=92%]\n",
      "[001/005 epoch valid]: 100% 2/2 [00:07<00:00,  3.98s/it, loss=5.051143, CUDA-Mem(%)=77, CUDA-Util(%)=99]\n",
      "[2024-07-02 13:06:15,788][cxrclip.trainer][INFO] - Epoch 1, last-model saved\n",
      "[2024-07-02 13:06:38,624][cxrclip.trainer][INFO] - Model/image-classification-model/runs/backbone/outputs/2024-07-02/12-25-42/checkpoints/model-best.tar saved\n",
      "[002/005 epoch train]: 100% 166/166 [17:38<00:00,  6.38s/it, lr=['0.00002677'], loss=3.181332, CUDA-Mem=82%, CUDA-Util=94%]\n",
      "[002/005 epoch valid]: 100% 2/2 [00:07<00:00,  3.76s/it, loss=5.076596, CUDA-Mem(%)=73, CUDA-Util(%)=99]\n",
      "[2024-07-02 13:24:36,731][cxrclip.trainer][INFO] - Epoch 2, last-model saved\n",
      "[003/005 epoch train]: 100% 166/166 [17:52<00:00,  6.46s/it, lr=['0.00000862'], loss=2.527943, CUDA-Mem=52%, CUDA-Util=55%]\n",
      "[003/005 epoch valid]: 100% 2/2 [00:07<00:00,  3.76s/it, loss=5.054688, CUDA-Mem(%)=73, CUDA-Util(%)=99]\n",
      "[2024-07-02 13:42:55,184][cxrclip.trainer][INFO] - Epoch 3, last-model saved\n",
      "[004/005 epoch train]:  99% 164/166 [18:00<00:13,  6.59s/it, lr=['0.00000006'], loss=2.413907, CUDA-Mem=81%, CUDA-Util=93%]\n",
      "[004/005 epoch valid]: 100% 2/2 [00:11<00:00,  5.84s/it, loss=4.969450, CUDA-Mem(%)=71, CUDA-Util(%)=97]\n",
      "[2024-07-02 14:01:40,400][cxrclip.trainer][INFO] - Epoch 4, last-model saved\n",
      "[2024-07-02 14:02:07,591][cxrclip.trainer][INFO] - Model/image-classification-model/runs/backbone/outputs/2024-07-02/12-25-42/checkpoints/model-best.tar saved\n",
      "[2024-07-02 14:02:07,666][cxrclip.trainer][INFO] - cuda: Training has been completed\n"
     ]
    }
   ],
   "source": [
    "!python \"Model/image-classification-model/train.py\" --config-name train \\\n",
    "  dataloader=dataloader_32 scheduler=cosine_epoch15_warmup1\n",
    "\n",
    "# output folder: Model/image-classification-model/runs/backbone/outputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153ae0d-83b6-4041-b05b-7e69173b61c5",
   "metadata": {
    "id": "a153ae0d-83b6-4041-b05b-7e69173b61c5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fadfd2b5-a63e-4849-984d-7d3b0083303c",
   "metadata": {
    "id": "fadfd2b5-a63e-4849-984d-7d3b0083303c"
   },
   "source": [
    "# 4.训练Finetune Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a56589-09ee-4e75-9c22-7852b12fac8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0a56589-09ee-4e75-9c22-7852b12fac8d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1719931667574,
     "user_tz": -60,
     "elapsed": 842063,
     "user": {
      "displayName": "Jianfei Xu",
      "userId": "14747087902840301965"
     }
    },
    "outputId": "28bc9556-ad48-48b3-afe4-bc301a5cf78d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2024-07-02 14:33:49.352350: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-02 14:33:49.352425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-02 14:33:49.354815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-02 14:33:49.365851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-02 14:33:51.567892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[2024-07-02 14:33:57,490][__main__][INFO] - Configurations:\n",
      "model:\n",
      "  load_backbone_weights: Model/image-classification-model/model/backbone/model-best.tar\n",
      "  freeze_backbone_weights: true\n",
      "  name: finetune_classification\n",
      "  image_encoder:\n",
      "    source: torchvision\n",
      "    name: resnet\n",
      "    pretrained: true\n",
      "  classifier:\n",
      "    config:\n",
      "      name: linear\n",
      "      n_class: 1\n",
      "base:\n",
      "  seed: 1234\n",
      "  amp: true\n",
      "  image_size: 224\n",
      "  loss_best: classification\n",
      "  data_frac: 1.0\n",
      "  output:\n",
      "    checkpoint: Model/image-classification-model/runs/finetune_100/outputs/2024-07-02/14-33-57/checkpoints/\n",
      "    tensorboard: Model/image-classification-model/runs/finetune_100/outputs/2024-07-02/14-33-57/tensorboard/\n",
      "data_train:\n",
      "  base:\n",
      "    name: rsna_pneumonia\n",
      "    data_type: image_classification\n",
      "    data_path: Model/image-classification-model/datasets/rsna_pneumonia/train.csv\n",
      "    image_size: 224\n",
      "    n_class: 1\n",
      "    data_frac: 1.0\n",
      "data_valid:\n",
      "  rsna_pneumonia:\n",
      "    name: rsna_pneumonia\n",
      "    data_type: image_classification\n",
      "    data_path: Model/image-classification-model/datasets/rsna_pneumonia/valid.csv\n",
      "    image_size: 224\n",
      "dataloader:\n",
      "  train:\n",
      "    pin_memory: true\n",
      "    shuffle: true\n",
      "    drop_last: true\n",
      "    num_workers: 8\n",
      "    prefetch_factor: 8\n",
      "    batch_size: 32\n",
      "  valid:\n",
      "    pin_memory: true\n",
      "    shuffle: false\n",
      "    drop_last: false\n",
      "    num_workers: 8\n",
      "    prefetch_factor: 8\n",
      "    batch_size: 32\n",
      "  test:\n",
      "    pin_memory: true\n",
      "    shuffle: false\n",
      "    drop_last: false\n",
      "    num_workers: 16\n",
      "    batch_size: 32\n",
      "transform:\n",
      "  train:\n",
      "    RandomResizedCrop:\n",
      "      size: 224\n",
      "      scale:\n",
      "      - 0.8\n",
      "      - 1.1\n",
      "    CLAHE:\n",
      "      clip_limit: 4.0\n",
      "    ColorJitter:\n",
      "      brightness: 0.1\n",
      "      contrast: 0.2\n",
      "      saturation: 0.2\n",
      "      hue: 0.1\n",
      "    CenterCrop:\n",
      "      size: 224\n",
      "  valid:\n",
      "    Resize:\n",
      "      size: 224\n",
      "    CenterCrop:\n",
      "      size: 224\n",
      "  test:\n",
      "    Resize:\n",
      "      size: 224\n",
      "    CenterCrop:\n",
      "      size: 224\n",
      "optimizer:\n",
      "  name: adamw\n",
      "  config:\n",
      "    lr: 5.0e-05\n",
      "    weight_decay: 0.0001\n",
      "scheduler:\n",
      "  name: cosine\n",
      "  config:\n",
      "    total_epochs: 5\n",
      "    warmup_epochs: 1\n",
      "loss:\n",
      "  classification:\n",
      "    loss_ratio: 1.0\n",
      "\n",
      "[2024-07-02 14:33:57,491][cxrclip.util.utils][INFO] - Global seed set to 1234\n",
      "[2024-07-02 14:33:57,540][cxrclip.trainer][INFO] - DistEnv: DistEnv(world_size=1, world_rank=0, local_rank=0, num_gpus=1, master=True, summary_writer=<cxrclip.util.global_env.SummaryWriter object at 0x7ee02a8157b0>)\n",
      "[2024-07-02 14:33:57,541][cxrclip.trainer][INFO] - cuda: Load datasets\n",
      "[2024-07-02 14:33:57,573][cxrclip.data.datamodule][INFO] - Dataset loaded: rsna_pneumonia for train\n",
      "[2024-07-02 14:33:57,588][cxrclip.data.datamodule][INFO] - Dataset loaded: rsna_pneumonia for valid\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "[2024-07-02 14:33:57,592][cxrclip.trainer][INFO] - cuda: Build the model\n",
      "[2024-07-02 14:33:57,592][cxrclip.model.image_classification][INFO] -     loading pre-trained image encoder for fine-tuning\n",
      "{'source': 'torchvision', 'name': 'resnet', 'pretrained': True}\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "[2024-07-02 14:34:02,393][cxrclip.model.image_classification][INFO] -     freezing image encoder to not be trained\n",
      "[2024-07-02 14:34:03,634][cxrclip.trainer][INFO] - cuda: Model info:\n",
      "CXRClassification(\n",
      "  (image_encoder): ResNet50(\n",
      "    (resnet): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (classifier): LinearClassifier(\n",
      "    (classification_head): Linear(in_features=2048, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "[2024-07-02 14:34:03,635][cxrclip.trainer][INFO] - cuda: Build the loss function\n",
      "[2024-07-02 14:34:03,635][cxrclip.trainer][INFO] - cuda: Build the optimizer\n",
      "[2024-07-02 14:34:03,636][cxrclip.trainer][INFO] - cuda: Build the LR scheulder\n",
      "[2024-07-02 14:34:03,637][cxrclip.trainer][INFO] - Download nltk module\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[2024-07-02 14:34:03,727][cxrclip.trainer][INFO] - cuda: Train the model\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "[000/005 epoch train]: 100% 218/218 [05:35<00:00,  1.54s/it, lr=['0.00004839'], loss=0.497839, CUDA-Mem=17%, CUDA-Util=31%]\n",
      "[000/005 epoch valid]: 100% 32/32 [00:52<00:00,  1.65s/it, loss=16.606504, CUDA-Mem(%)=21, CUDA-Util(%)=35]\n",
      "[2024-07-02 14:40:33,959][cxrclip.trainer][INFO] - Epoch 0, last-model saved\n",
      "[2024-07-02 14:40:34,398][cxrclip.trainer][INFO] - Model/image-classification-model/runs/finetune_100/outputs/2024-07-02/14-33-57/checkpoints/model-best.tar saved\n",
      "[001/005 epoch train]: 100% 218/218 [01:35<00:00,  2.28it/s, lr=['0.00004312'], loss=0.484030, CUDA-Mem=35%, CUDA-Util=39%]\n",
      "[001/005 epoch valid]: 100% 32/32 [00:12<00:00,  2.61it/s, loss=15.038536, CUDA-Mem(%)=78, CUDA-Util(%)=97]\n",
      "[2024-07-02 14:42:23,906][cxrclip.trainer][INFO] - Epoch 1, last-model saved\n",
      "[2024-07-02 14:42:24,582][cxrclip.trainer][INFO] - Model/image-classification-model/runs/finetune_100/outputs/2024-07-02/14-33-57/checkpoints/model-best.tar saved\n",
      "[002/005 epoch train]: 100% 218/218 [01:36<00:00,  2.26it/s, lr=['0.00002563'], loss=0.452847, CUDA-Mem=21%, CUDA-Util=37%]\n",
      "[002/005 epoch valid]: 100% 32/32 [00:07<00:00,  4.38it/s, loss=14.344246, CUDA-Mem(%)=79, CUDA-Util(%)=99]\n",
      "[2024-07-02 14:44:09,725][cxrclip.trainer][INFO] - Epoch 2, last-model saved\n",
      "[2024-07-02 14:44:10,249][cxrclip.trainer][INFO] - Model/image-classification-model/runs/finetune_100/outputs/2024-07-02/14-33-57/checkpoints/model-best.tar saved\n",
      "[003/005 epoch train]: 100% 218/218 [01:37<00:00,  2.23it/s, lr=['0.00000777'], loss=0.343640, CUDA-Mem=50%, CUDA-Util=86%]\n",
      "[003/005 epoch valid]: 100% 32/32 [00:09<00:00,  3.22it/s, loss=14.139630, CUDA-Mem(%)=55, CUDA-Util(%)=90]\n",
      "[2024-07-02 14:45:59,276][cxrclip.trainer][INFO] - Epoch 3, last-model saved\n",
      "[2024-07-02 14:46:00,077][cxrclip.trainer][INFO] - Model/image-classification-model/runs/finetune_100/outputs/2024-07-02/14-33-57/checkpoints/model-best.tar saved\n",
      "[004/005 epoch train]:  99% 216/218 [01:35<00:00,  2.27it/s, lr=['0.00000001'], loss=0.304964, CUDA-Mem=26%, CUDA-Util=44%]\n",
      "[004/005 epoch valid]: 100% 32/32 [00:07<00:00,  4.49it/s, loss=14.113969, CUDA-Mem(%)=78, CUDA-Util(%)=98]\n",
      "[2024-07-02 14:47:44,148][cxrclip.trainer][INFO] - Epoch 4, last-model saved\n",
      "[2024-07-02 14:47:44,673][cxrclip.trainer][INFO] - Model/image-classification-model/runs/finetune_100/outputs/2024-07-02/14-33-57/checkpoints/model-best.tar saved\n",
      "[2024-07-02 14:47:44,687][cxrclip.trainer][INFO] - cuda: Training has been completed\n"
     ]
    }
   ],
   "source": [
    "!python \"Model/image-classification-model/finetune.py\" --config-name finetune_100 \\\n",
    "  data_train=rsna_pneumonia data_valid=rsna_pneumonia \\\n",
    "  dataloader=dataloader_32 scheduler=cosine_epoch5_warmup1 \\\n",
    "  model.load_backbone_weights=\"Model/image-classification-model/model/backbone/model-best.tar\"\n",
    "\n",
    "# output folder: Model/image-classification-model/runs/finetune_10/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cddd4e-0955-4fd1-a0e0-934e4412a272",
   "metadata": {
    "id": "d4cddd4e-0955-4fd1-a0e0-934e4412a272"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f200d129-6bd2-4e16-b8d7-c16cc20abc2c",
   "metadata": {
    "id": "f200d129-6bd2-4e16-b8d7-c16cc20abc2c"
   },
   "source": [
    "# 5.评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b8871ed-ccff-4a97-b10d-49789b90c048",
   "metadata": {
    "id": "9b8871ed-ccff-4a97-b10d-49789b90c048",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1719931844626,
     "user_tz": -60,
     "elapsed": 91775,
     "user": {
      "displayName": "Jianfei Xu",
      "userId": "14747087902840301965"
     }
    },
    "outputId": "398e5234-d73e-49e5-db48-f067b98065d0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2024-07-02 14:49:15.777554: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-02 14:49:15.777606: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-02 14:49:15.779234: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-02 14:49:15.787236: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-02 14:49:16.883780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Model/image-classification-model/model/backbone/model-best.tar\n",
      "Global seed set to 1234\n",
      "{'base': {'seed': 1234, 'amp': True, 'image_size': 224, 'text_max_length': 256, 'loss_best': 'contrastive', 'data_frac': 1.0, 'output': {'checkpoint': 'Model/image-classification-model/runs/backbone/outputs/2024-07-02/12-25-42/checkpoints/', 'tensorboard': 'Model/image-classification-model/runs/backbone/outputs/2024-07-02/12-25-42/tensorboard/'}}, 'data_train': {'mimic_cxr': {'name': 'mimic_cxr', 'data_type': 'imagetext', 'data_path': 'Model/image-classification-model/datasets/mimic-cxr-jpg/mimic_train.csv', 'image_size': 224, 'text_max_length': 256, 'data_frac': 1.0, 'normalize': 'imagenet'}}, 'data_valid': {'mimic_cxr': {'name': 'mimic_cxr', 'data_type': 'imagetext', 'data_path': 'Model/image-classification-model/datasets/mimic-cxr-jpg/mimic_valid.csv', 'image_size': 224, 'text_max_length': 256, 'normalize': 'imagenet'}}, 'dataloader': {'train': {'pin_memory': True, 'shuffle': True, 'drop_last': True, 'num_workers': 8, 'prefetch_factor': 8, 'batch_size': 32}, 'valid': {'pin_memory': True, 'shuffle': False, 'drop_last': False, 'num_workers': 8, 'prefetch_factor': 8, 'batch_size': 32}, 'test': {'pin_memory': True, 'shuffle': False, 'drop_last': False, 'num_workers': 16, 'batch_size': 32}}, 'tokenizer': {'source': 'huggingface', 'pretrained_model_name_or_path': 'emilyalsentzer/Bio_ClinicalBERT', 'cache_dir': 'huggingface/tokenizers'}, 'transform': {'train': {'RandomResizedCrop': {'size': 224, 'scale': [0.8, 1.1]}, 'CLAHE': {'clip_limit': 4.0}, 'ColorJitter': {'brightness': 0.1, 'contrast': 0.2, 'saturation': 0.2, 'hue': 0.1}, 'CenterCrop': {'size': 224}}, 'valid': {'Resize': {'size': 224}, 'CenterCrop': {'size': 224}}, 'test': {'Resize': {'size': 224}, 'CenterCrop': {'size': 224}}}, 'model': {'name': 'clip_custom', 'temperature': 0.07, 'image_encoder': {'source': 'torchvision', 'name': 'resnet', 'pretrained': True}, 'text_encoder': {'source': 'huggingface', 'name': 'emilyalsentzer/Bio_ClinicalBERT', 'pretrained': True, 'gradient_checkpointing': False, 'pooling': 'eos', 'cache_dir': 'huggingface/', 'trust_remote_code': True, 'mlm_head': True}, 'projection_head': {'name': 'linear', 'dropout': 0.1, 'proj_dim': 512}}, 'optimizer': {'name': 'adamw', 'config': {'lr': 5e-05, 'weight_decay': 0.0001}}, 'scheduler': {'name': 'cosine', 'config': {'total_epochs': 5, 'warmup_epochs': 1, 'total_steps': 830, 'warmup_steps': 166}}, 'loss': {'cxr_clip': {'label_smoothing': 0.0, 'i2i_weight': 1.0, 't2t_weight': 0.5, 'loss_ratio': 1.0}}}\n",
      "Dataset loaded: rsna_pneumonia for test\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "rsna_pneumonia\n",
      "Load model Model/image-classification-model/model/backbone/model-best.tar\n",
      "  0% 0/63 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "100% 63/63 [01:01<00:00,  1.02it/s]\n",
      "evaluate zero-shot binary classification\n",
      "Pneumonia: {'AUROC': 0.544136554265109, 'Accuracy': 0.549, 'F1': 0.3207831325301205}\n",
      "AUROC(Avg): 0.544136554265109\n",
      "F1(Avg): 0.3207831325301205\n",
      "Accuracy(Avg): 0.549\n",
      "print best score\n",
      "Model/image-classification-model/model/backbone/model-best.tar\n",
      "\n",
      "Zeroshot binary\n",
      "| model | AUROC(Avg) | F1(Avg) | Accuracy(Avg)|\n",
      "| :---- | ---------: | ------: | ------------:|\n",
      "| model-best | 0.544 | 0.321 | 0.549 |\n",
      "Best AUROC(Avg): 0.544, from model-best\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python \"Model/image-classification-model/evaluate_clip.py\" dataloader=dataloader_32 \\\n",
    "  test.checkpoint=\"Model/image-classification-model/model/backbone/model-best.tar\"\n",
    "\n",
    "# output folder: Model/image-classification-model/evaluation/backbone/outputs"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "eK_C8qB6SVjR"
   },
   "id": "eK_C8qB6SVjR",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!python \"Model/image-classification-model/evaluate_finetune.py\" dataloader=dataloader_32 \\\n",
    "  test.checkpoint=\"Model/image-classification-model/model/finetune/model-best.tar\"\n",
    "\n",
    "# output folder: Model/image-classification-model/evaluation/finetune/outputs"
   ],
   "metadata": {
    "id": "lxNo0h0F_Sqn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1719931896956,
     "user_tz": -60,
     "elapsed": 52335,
     "user": {
      "displayName": "Jianfei Xu",
      "userId": "14747087902840301965"
     }
    },
    "outputId": "ff36eaf2-8aea-4633-bd74-475f36af5dd0"
   },
   "id": "lxNo0h0F_Sqn",
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2024-07-02 14:50:50.763337: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-02 14:50:50.763399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-02 14:50:50.764906: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-02 14:50:50.772461: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-02 14:50:52.645065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Model/image-classification-model/model/finetune/model-best.tar\n",
      "Global seed set to 1234\n",
      "{'model': {'load_backbone_weights': 'Model/image-classification-model/model/backbone/model-best.tar', 'freeze_backbone_weights': True, 'name': 'finetune_classification', 'image_encoder': {'source': 'torchvision', 'name': 'resnet', 'pretrained': True}, 'classifier': {'config': {'name': 'linear', 'n_class': 1}}}, 'base': {'seed': 1234, 'amp': True, 'image_size': 224, 'loss_best': 'classification', 'data_frac': 1.0, 'output': {'checkpoint': 'Model/image-classification-model/runs/finetune_100/outputs/2024-07-02/14-33-57/checkpoints/', 'tensorboard': 'Model/image-classification-model/runs/finetune_100/outputs/2024-07-02/14-33-57/tensorboard/'}}, 'data_train': {'base': {'name': 'rsna_pneumonia', 'data_type': 'image_classification', 'data_path': 'Model/image-classification-model/datasets/rsna_pneumonia/train.csv', 'image_size': 224, 'n_class': 1, 'data_frac': 1.0, 'normalize': 'imagenet'}}, 'data_valid': {'rsna_pneumonia': {'name': 'rsna_pneumonia', 'data_type': 'image_classification', 'data_path': 'Model/image-classification-model/datasets/rsna_pneumonia/valid.csv', 'image_size': 224, 'normalize': 'imagenet'}}, 'dataloader': {'train': {'pin_memory': True, 'shuffle': True, 'drop_last': True, 'num_workers': 8, 'prefetch_factor': 8, 'batch_size': 32}, 'valid': {'pin_memory': True, 'shuffle': False, 'drop_last': False, 'num_workers': 8, 'prefetch_factor': 8, 'batch_size': 32}, 'test': {'pin_memory': True, 'shuffle': False, 'drop_last': False, 'num_workers': 16, 'batch_size': 32}}, 'transform': {'train': {'RandomResizedCrop': {'size': 224, 'scale': [0.8, 1.1]}, 'CLAHE': {'clip_limit': 4.0}, 'ColorJitter': {'brightness': 0.1, 'contrast': 0.2, 'saturation': 0.2, 'hue': 0.1}, 'CenterCrop': {'size': 224}}, 'valid': {'Resize': {'size': 224}, 'CenterCrop': {'size': 224}}, 'test': {'Resize': {'size': 224}, 'CenterCrop': {'size': 224}}}, 'optimizer': {'name': 'adamw', 'config': {'lr': 5e-05, 'weight_decay': 0.0001}}, 'scheduler': {'name': 'cosine', 'config': {'total_epochs': 5, 'warmup_epochs': 1, 'total_steps': 1090, 'warmup_steps': 218}}, 'loss': {'classification': {'loss_ratio': 1.0}}}\n",
      "Dataset loaded: rsna_pneumonia for test\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "    loading pre-trained image encoder for fine-tuning\n",
      "{'source': 'torchvision', 'name': 'resnet', 'pretrained': True}\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "    freezing image encoder to not be trained\n",
      "rsna_pneumonia\n",
      "Load model Model/image-classification-model/model/finetune/model-best.tar\n",
      "  0% 0/63 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "100% 63/63 [00:24<00:00,  2.58it/s]\n",
      "evaluate multi-label classification\n",
      "Pneumonia: {'AUROC': 0.8477349166870009, 'Accuracy': 0.7805, 'F1': 0.13069306930693067}\n",
      "AUROC(Avg): 0.8477349166870009\n",
      "F1(Avg): 0.13069306930693067\n",
      "Accuracy(Avg): 0.7805\n",
      "print best score\n",
      "Model/image-classification-model/model/finetune/model-best.tar\n",
      "| model | AUROC(Avg) | F1(Avg) | Accuracy(Avg)|\n",
      "| :---- | ---------: | ------: | ------------:|\n",
      "| model-best | 0.848 | 0.131 | 0.780 |\n",
      "\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "9qBJGvf-wzj6",
    "6c053a87-826c-4043-8b5d-f3e41914335b",
    "ceb384f4-5798-4ca7-8f43-f05f737865c6",
    "d739a7ad-9d68-40dd-a9b3-c1eb0fc55e06",
    "fadfd2b5-a63e-4849-984d-7d3b0083303c"
   ],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
